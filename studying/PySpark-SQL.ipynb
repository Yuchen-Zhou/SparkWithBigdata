{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fcaf78a",
   "metadata": {},
   "source": [
    "# Spark SQL\n",
    "## Spark SQL简介\n",
    "### Shark\n",
    "Shark即Hive on Spark，为了实现与Hive兼容，Shark在HiveQL方面重用了Hive中HiveQL的解析、逻辑执行计划翻译、执行计划优化等逻辑，可以近似认为仅将物理执行计划从MapReduce作业替换为Spark作业，通过Hive的HiveQL解析，把HiveQL翻译成Spark上的RDD操作\n",
    "\n",
    "Shark的设计导致了两个问题\n",
    "- 执行计划优化完全依赖于Hive，不方便添加新的优化策略\n",
    "- Spark是线程级并行，而MapReduce是进程级并行，因此，Shark在兼容Hive的实现上存在线程安全问题\n",
    "\n",
    "\n",
    "### Spark SQL设计\n",
    "Spark SQL在Hive兼容层面仅依赖HiveQL解析、Hive元数据。从HQL被解析成抽象语法树（AST）起，就全部由Spark SQL接管了。Spark SQL执行计划生成和优化都由Catalyst负责\n",
    "\n",
    "- Spark SQL增加了DataFrame（即带有Schema信息的RDD），使用户可以在Spark SQL中执行SQL语句，数据即可以来自RDD，也可以是Hive、HDFS、Cassandra等外部数据源，还可以是JSON格式数据\n",
    "\n",
    "\n",
    "### 为什么推出Spark SQL\n",
    "- Spark SQL提供了DataFrame API，可以对内部和外部各种数据源执行各种关系型操作\n",
    "- 其次，可以支持大数据中的大量数据源和数据分析算法。Spark SQL可以融合：传统关系数据库的结构化管理能力和机器学习算法的数据处理能力\n",
    "\n",
    "\n",
    "## DataFrame概述\n",
    "- DataFrame的推出，让Spark具备了处理大规模结构化数据的能力，不仅比原有的RDD转化方式更加简单易用，而且有更高的计算性能\n",
    "- Spark能够轻松实现从MySQL到DataFrame的转化，并且支持SQL查询\n",
    "\n",
    "\n",
    "**DataFrame与RDD的区别**\n",
    "- RDD是分布式的Java对象的集合，但是，对象内部结构对RDD而言却不可知\n",
    "- DataFrame是一种以RDD为基础的分布式数据集，提供了详细的结构信息"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d29b6f",
   "metadata": {},
   "source": [
    "## DataFrame的创建\n",
    "- 从Spark2.0以上版本开始，Spark使用全新的SparkSession接口\n",
    "- SparkSeesion支持从不同的数据源加载数据，并把数据转换成DataFrame，并且支持把DataFrame转换成SQLContext自身中的表，然后使用SQL语句来操作数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d54f101c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.config(conf = SparkConf()).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb18d450",
   "metadata": {},
   "source": [
    "在创建DataFrame时，可以使用spark.read操作，从不同类型的文件中加载数据创建DF，例如：\n",
    "- spark.read.text('people.txt'):读取文本文件people.txt创建DF\n",
    "- spark.read.json('people.txt'):读取people.json文件创建DF；在本地读取时要给出正确的路径\n",
    "- spark.read.parquet('people.parquet'):读取people.parquet文件创建DF\n",
    "\n",
    "或者\n",
    "\n",
    "- spark.read.format(\"text\").load(\"people.txt\")：读取文本文件people.json创建DataFrame；\n",
    "- spark.read.format(\"json\").load(\"people.json\")：读取JSON文件people.json创建DataFrame；\n",
    "- spark.read.format(\"parquet\").load(\"people.parquet\")：读取Parquet文件people.parquet创建DataFrame。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45a22476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json('file:///opt/code/SparkProgramming-PySpark/resources/people.json')\n",
    "# df = spark.read.text('file:///opt/code/SparkProgramming-PySpark/resources/people.txt')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae392af",
   "metadata": {},
   "source": [
    "## DataFrame的保存\n",
    "可以使用spark.write操作，把一个DataFrame保存成不同格式的文件，例如，把一个名称为df的DataFrame保存到不同格式文件中\n",
    "- df.write.text('people.txt')\n",
    "- df.write.json('people.json')\n",
    "- df.write.parquet('people.parquet')\n",
    "\n",
    "或者\n",
    "\n",
    "- df.write.format(\"text\").save(\"people.txt\")\n",
    "- df.write.format(\"json\").save(\"people.json\")\n",
    "- df.write.format (\"parquet\").save(\"people.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeb44b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleDF = spark.read.format('json').load('file:///opt/code/SparkProgramming-PySpark/resources/people.json')\n",
    "peopleDF.select('name', 'age').write.format('json').\\\n",
    "    save('file:///opt/code/SparkProgramming-PySpark/resources/newpeople.json')\n",
    "\n",
    "# peopleDF.select('name', 'age').write.format('text').\\\n",
    "#     save('file:///opt/code/SparkProgramming-PySpark/resources/newpeople.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed304f58",
   "metadata": {},
   "source": [
    "## DataFrame的常用操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f572045",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json('resources/people.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7df507c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#printSchema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60c87606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|   name| age|\n",
      "+-------+----+\n",
      "|Michael|null|\n",
      "|   Andy|  30|\n",
      "| Justin|  19|\n",
      "+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#select()\n",
    "df.select(df['name'], df['age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4802a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#filter()\n",
    "df.filter(df['age']>20).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ab1a8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| age|count|\n",
      "+----+-----+\n",
      "|  19|    1|\n",
      "|null|    1|\n",
      "|  30|    1|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#groupBy\n",
    "df.groupBy('age').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bae0f888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "|null|Michael|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sort()\n",
    "df.sort(df['age'].desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f5130ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "|null|Michael|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(df['age'].desc(), df['name'].desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72977d98",
   "metadata": {},
   "source": [
    "## 从RDD转换得到DataFrame\n",
    "### 利用反射机制推断RDD模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcd0b109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name: Michael,Age: 29', 'Name: Andy,Age: 30']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import Row, SparkSession\n",
    "\n",
    "people = spark.sparkContext.textFile('file:///opt/software/spark/examples/src/main/resources/people.txt')\\\n",
    "    .map(lambda line: line.split(',')).map(lambda p: Row(name=p[0], age=int(p[1])))\n",
    "schemaPeople = spark.createDataFrame(people)\n",
    "schemaPeople.createOrReplaceTempView('people')\n",
    "personsDF = spark.sql('select name,age from people where age > 20')\n",
    "personsRDD = personsDF.rdd.map(lambda p: 'Name: '+ p.name + ',' + 'Age: '+ str(p.age))\n",
    "personsRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2131a6e7",
   "metadata": {},
   "source": [
    "#### 使用编程方式定义RDD模式\n",
    "无法提前获取数据结构时，就需要采用编程的方式定义RDD模式\n",
    "比如，需要通过编程方式把people.txt加载进来生成DataFrame，并完成SQL查询"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09be7b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "\n",
    "schemaString = 'name age'\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split(' ')]\n",
    "schema = StructType(fields)\n",
    "lines = spark.sparkContext.textFile('file:///opt/software/spark/examples/src/main/resources/people.txt')\n",
    "parts = lines.map(lambda x: x.split(','))\n",
    "people = parts.map(lambda p: Row(p[0], p[1].strip()))\n",
    "schemaPeople = spark.createDataFrame(people, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a58dede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|Michael| 29|\n",
      "|   Andy| 30|\n",
      "| Justin| 19|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schemaPeople.createOrReplaceTempView('people')\n",
    "results = spark.sql('select name, age from people')\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbbedca",
   "metadata": {},
   "source": [
    "## 使用Spark SQL读写数据库\n",
    "### 准备工作\n",
    "- 启动MySQL\n",
    "- 创建数据库和表\n",
    "```mysql\n",
    "create database spark;\n",
    "use spark;\n",
    "create table student (id int(4), name char(20), gender char(4), age int(4));\n",
    "insert into student values(1, 'Xueqian', 'F', 23);\n",
    "insert into student values(2, 'Weiliang', 'M', 24);\n",
    "select * from student;\n",
    "```\n",
    "\n",
    "- 将JDBC驱动拷贝到$SPARK_HOME/jars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451b2161",
   "metadata": {},
   "source": [
    "### 读取MySQL数据库中的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46a1f653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+---+\n",
      "| id|    name|gender|age|\n",
      "+---+--------+------+---+\n",
      "|  1| Xueqian|     F| 23|\n",
      "|  2|Weiliang|     M| 25|\n",
      "+---+--------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf().setMaster('local').setAppName('readMysql')\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "jdbcDF = spark.read.format('jdbc')\\\n",
    "    .option('driver', 'com.mysql.jdbc.Driver')\\\n",
    "    .option('url', 'jdbc:mysql://localhost:3306/spark')\\\n",
    "    .option('dbtable', 'student')\\\n",
    "    .option('user', 'root')\\\n",
    "    .option('password', 'Passwd1!').load()\n",
    "jdbcDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d25d101",
   "metadata": {},
   "source": [
    "### 向MySQL数据库写入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5703b6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.config(conf= SparkConf()).getOrCreate()\n",
    "\n",
    "#下面设置模式信息\n",
    "schema = StructType([StructField('id', IntegerType(), True), \\\n",
    "                    StructField('name', StringType(), True),\\\n",
    "                    StructField('gender', StringType(), True),\\\n",
    "                    StructField('age', IntegerType(), True)])\n",
    "\n",
    "#设置两条数据，表示学生信息\n",
    "studentRDD = spark.sparkContext.parallelize(['3 Rongcheng M 26', '4 Guanhua M 27'])\\\n",
    "                        .map(lambda x:x.split(' '))\n",
    "\n",
    "#创建Row对象，每个Row对象都是rowRDD中的一行\n",
    "rowRDD = studentRDD.map(lambda p: Row(int(p[0].strip()), p[1].strip(), p[2].strip(), int(p[3].strip())))\n",
    "\n",
    "#建立Row对象和模式之间的对应关系，也就是把数据和模式对应起来\n",
    "studentDF = spark.createDataFrame(rowRDD, schema)\n",
    "\n",
    "#写入数据库\n",
    "prop = {}\n",
    "prop['user'] = 'root'\n",
    "prop['password'] = 'Passwd1!'\n",
    "prop['driver'] = 'com.mysql.jdbc.Driver'\n",
    "studentDF.write.jdbc('jdbc:mysql://localhost:3306/spark', 'student', 'append', prop)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
