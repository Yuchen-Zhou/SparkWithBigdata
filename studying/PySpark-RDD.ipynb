{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dce35bf",
   "metadata": {},
   "source": [
    "# RDD编程\n",
    "## RDD编程基础"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b9ffe7",
   "metadata": {},
   "source": [
    " ### RDD创建\n",
    " 1.从文件系统中加载数据创建RDD \n",
    " - Spark采用textFile()方法来从文件系统中加载数据创建RDD\n",
    " - 该方法把文件的URI作为参数，这个URI可以是\n",
    "     - 本地文件系统的地址\n",
    "     - 或者是分布式文件系统HDFS的地址\n",
    "     - 或者是Amazon S3的地址等等\n",
    "     \n",
    "(1)从本地文件系统中加载数据创建RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55bfcc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf().setMaster('local')\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a594e396",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile('file:///opt/code/SparkProgramming-PySpark/word.txt')\n",
    "lines.foreach(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90cf93fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: -c: line 0: syntax error near unexpected token `('\r\n",
      "/bin/bash: -c: line 0: `lines = sc.textFile('file:///opt/code/SparkProgramming-PySpark/word.txt')'\r\n"
     ]
    }
   ],
   "source": [
    "!lines = sc.textFile('file:///opt/code/SparkProgramming-PySpark/word.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08a6e3e",
   "metadata": {},
   "source": [
    "此代码的输出可以在Jupyter-notebook终端看到"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dbde1b",
   "metadata": {},
   "source": [
    "(2)从分布式文件系统HDFS中加载数据\n",
    "\n",
    "```\n",
    "lines = sc.textFile('hdfs://localhost:9000/user/spark/stocks/part-m-00000')\n",
    "lines = sc.textFile('/user/spark/stocks/part-m-00000')\n",
    "lines = sc.textFile('stocks/part-m-00000')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbb722f",
   "metadata": {},
   "source": [
    "2.通过并行集合（列表）创建RDD  \n",
    "可以调用SparkContext的parallelize方法，在Driver中一个已经存在的集合（列表）上创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "034310dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(array)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862b7746",
   "metadata": {},
   "source": [
    "### RDD操作\n",
    "1.转换操作\n",
    "- 对于RDD而言，每一次转换操作都会产生不同的RDD，供给下一个“转换”使用\n",
    "- 转换得到的RDD是惰性求值，整个转换过程只是记录了转换的轨迹，并不会发生真正的计算，只有遇到行动操作时，才会发生真正的计算\n",
    "\n",
    "| 操作              | 含义                                                         |\n",
    "      | ----------------- | ------------------------------------------------------------ |\n",
    "      | filter(func)      | 筛选出满足函数func的元素，并返回一个新的数据集               |\n",
    "      | map(func)         | 将每个元素传递到函数func中，并将结果返回为一个新的数据集     |\n",
    "      | flatMap(func)     | 与map相似(),但每个输入元素都可以映射到0或多个输出结果        |\n",
    "      | groupByKey()      | 应用于(K, V)键值对的数据集时，返回一个新的(K, Iterable)形式的数据集 |\n",
    "      | reduceByKey(func) | 应用于(K, V)键值对的数据集时，返回一个新的(K, V)形式的数据集，其中每个值都是将每个key传递到函数func中进行聚合后的结果 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e31bd4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark is fast\n",
      "Spark is better\n"
     ]
    }
   ],
   "source": [
    "#filter(func)\n",
    "lines = sc.textFile('file:///opt/code/SparkProgramming-PySpark/word.txt')\n",
    "linesWithSpark = lines.filter(lambda line: \"Spark\" in line)\n",
    "for i in linesWithSpark.collect():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ce41755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "#map(func)\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd1 = sc.parallelize(data)\n",
    "rdd2 = rdd1.map(lambda x: x+10)\n",
    "for r in rdd.collect():\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83a536e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hadoop', 'is', 'good']\n",
      "['Spark', 'is', 'fast']\n",
      "['Spark', 'is', 'better']\n"
     ]
    }
   ],
   "source": [
    "#map(func) 另一个实例\n",
    "lines = sc.textFile('file:///opt/code/SparkProgramming-PySpark/word.txt')\n",
    "words = lines.map(lambda line: line.split(\" \"))\n",
    "for word in words.collect():\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fd099b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatMap(func)\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e7e0534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Hadoop', 1)\n",
      "('is', 1)\n",
      "('good', 1)\n",
      "('Spark', 1)\n",
      "('is', 1)\n",
      "('fast', 1)\n",
      "('Spark', 1)\n",
      "('is', 1)\n",
      "('better', 1)\n"
     ]
    }
   ],
   "source": [
    "#groupByKey()\n",
    "words = sc.parallelize([(\"Hadoop\", 1), (\"is\", 1), (\"good\", 1), (\"Spark\", 1), (\"is\", 1), (\"fast\", 1), (\"Spark\", 1),\n",
    "                       (\"is\", 1), (\"better\", 1)])\n",
    "words1 = words.groupByKey()\n",
    "for w in words.collect():\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf78f3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Hadoop', 1)\n",
      "('is', 3)\n",
      "('good', 1)\n",
      "('Spark', 2)\n",
      "('fast', 1)\n",
      "('better', 1)\n"
     ]
    }
   ],
   "source": [
    "#reduceByKey(func)\n",
    "words2 = words.reduceByKey(lambda a, b: a + b)\n",
    "for w in words2.collect():\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd583e6",
   "metadata": {},
   "source": [
    "2.行动操作\n",
    "行动操作是真正触发计算的地方\n",
    "\n",
    "|操作          | 含义                                                     |\n",
    "      | ------------- | -------------------------------------------------------- |\n",
    "      | count()       | 返回数据集中的元素个数                                   |\n",
    "      | collect()     | 以数组的形式返回数据集中的所有元素                       |\n",
    "      | first()       | 返回数据集中的第一个元素                                 |\n",
    "      | take(n)       | 以数组的形式返回数据集中的前n个元素                      |\n",
    "      | reduce(func)  | 通过函数func(输入两个参数并返回一个值)聚合数据集中的元素 |\n",
    "      | foreach(func) | 将数据集中的每个元素传递到函数func中运行                 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "940c3263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "1\n",
      "[1, 2, 3]\n",
      "[1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "print(rdd.count())\n",
    "print(rdd.first())\n",
    "print(rdd.take(3))\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed87f8b",
   "metadata": {},
   "source": [
    "3.惰性机制  \n",
    "惰性机制是指，整个转换过程只是记录了转换的轨迹，并不会发生真正的计算，只有遇到行动操作时，才会触发真正的计算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04b4195",
   "metadata": {},
   "source": [
    "### 持久化  \n",
    "在Spark中，RDD采用惰性求值的机制，每次遇到行动操作，都会从头开始执行计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9437aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Hadoop,Spark,Hive\n"
     ]
    }
   ],
   "source": [
    "list = [\"Hadoop\", \"Spark\", \"Hive\"]\n",
    "rdd = sc.parallelize(list)\n",
    "print(rdd.count())\n",
    "print(','.join(rdd.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e5efe6",
   "metadata": {},
   "source": [
    "- 可以通过持久化（缓存）机制避免这种重复计算的开销\n",
    "- 可以使用persist()方法对一个RDD标记为持久化\n",
    "- 之所以说“标记为持久化”，是因为出现persist()语句的地方，并不会马上计算生成RDD并把它持久化，而是要等到遇到第一个行动操作触发真正计算以后，才会把计算结果进行持久化\n",
    "- 持久化后的RDD将会被保留在计算节点的内存中被后面的行动操作重复使用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e96c6e",
   "metadata": {},
   "source": [
    "实际上，可以通过持久化（缓存）机制来避免这种重复计算的开销。具体方法是使用persist()方法对一个RDD标记为持久化，持久化后的RDD会被保留到计算节点的内存中，被后面的行动操作重复使用。\n",
    "\n",
    "- persist(MEMORY_ONLY):表示将RDD作为反序列化的对象存储在JVM中，如果内存不足，就要按照LRU原则替换缓存中的内容\n",
    "- persist(MEMORY_AND_DISK):表示将RDD作为反序列化的对象存储在JVM中，如果内存不足，超出的分区将会被存放在磁盘中\n",
    "- 可以使用unpersist()方法手动地把持久化的RDD从缓存中移除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbe1116e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Hadoop,Spark,Hive\n"
     ]
    }
   ],
   "source": [
    "list = [\"Hadoop\", \"Spark\", \"Hive\"]\n",
    "rdd = sc.parallelize(list)\n",
    "rdd.cache() #这时候会调用persist(MEMORY_ONLY)，但是，语句到这里并不会缓存RDD\n",
    "print(rdd.count()) #第一次行动操作，触发一次真正从头到尾的计算，把上面的rdd放入缓存\n",
    "print(','.join(rdd.collect())) #第二次行动操作，重复使用上面的rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7555641f",
   "metadata": {},
   "source": [
    "### 分区\n",
    "\n",
    "- 分区的作用\n",
    "\n",
    "  RDD是弹性分布式数据集，通常RDD很大，会被分成多个分区，分别保存在不同的节点上。\n",
    "\n",
    "  对RDD进行分区，第一个功能是增加并行度，第二个功能是减少通信开销\n",
    "\n",
    "- 分区的原则\n",
    "\n",
    "  RDD分区的一个原则是使分区的个数尽量等于集群中的CPU核心(Core)数目。对于不同的Spark部署模式而言(Local模式、Standalone模式、YARN模式、Mesos模式)，都可以通过设置spark.default.parallelism这个参数的值，来配置默认的分区数目。\n",
    "\n",
    "  - Local模式：默认为本地机器的CPU数目，若设置了local[N]，则默认为N\n",
    "  - Standalone或YARN模式：在“集群中所有CPU核心数目总和”和“2”这二者中取较大值作为默认值\n",
    "  - Mesos模式：默认的分区数为8\n",
    "\n",
    "- 设置分区的个数\n",
    "    - 创建RDD时手动指定分区个数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4531b178",
   "metadata": {},
   "outputs": [],
   "source": [
    "list = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(list, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77eb5bb0",
   "metadata": {},
   "source": [
    "- 使用repartition方法重新设置分区个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ee9f917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "data = sc.parallelize(list, 2)\n",
    "print(len(data.glom().collect()))\n",
    "rdd = data.repartition(1)\n",
    "print(len(rdd.glom().collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4630c9a2",
   "metadata": {},
   "source": [
    "- 自定义分区方法\n",
    "\n",
    "Spark提供了自带的哈希分区(HashPartitioner)与区域分区(RangePartitioner)，能够满足大多数应用场景的需求。与此同时，Spark也支持自定义方式，即通过提供一个自定义的Partitioner对象来控制RDD的分区方式，从而利用领域知识进一步减小通信开销。\n",
    "\n",
    "实现自定义分区的方法：\n",
    "\n",
    "- numPartitions: Int \t返回创建出来的分区数\n",
    "- getPartition(Key: Any): Int  返回给定键的分区编号（0 到 numPartitioners-1）\n",
    "- equals():  Java判断相等性的标准方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b138dfae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main func is running\n"
     ]
    }
   ],
   "source": [
    "#这是一个实例\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "def MyPartitioner(key):\n",
    "    print(\"My Partitioner is running\")\n",
    "    print('The key is %d' % key)\n",
    "    return key % 10\n",
    "\n",
    "def main():\n",
    "    print(\"The main func is running\")\n",
    "#     conf = SparkConf().setMaster('local').setAppName('App')\n",
    "#     sc = SparkContext(conf=conf)\n",
    "    data = sc.parallelize(range(10), 5)\n",
    "    data.map(lambda x: (x, 1))\\\n",
    "    .partitionBy(10, MyPartitioner)\\\n",
    "    .map(lambda x: x[0])\\\n",
    "    .saveAsTextFile('file:///opt/code/SparkProgramming-PySpark/partitioner')\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a88e881",
   "metadata": {},
   "source": [
    "MyPartitioner的调用输出在终端显示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8f8fd3",
   "metadata": {},
   "source": [
    "## 键值对RDD\n",
    "### 键值对RDD的创建\n",
    "\n",
    "1.从文件中加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b940974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Hadoop', 1)\n",
      "('is', 1)\n",
      "('good', 1)\n",
      "('Spark', 1)\n",
      "('is', 1)\n",
      "('fast', 1)\n",
      "('Spark', 1)\n",
      "('is', 1)\n",
      "('better', 1)\n"
     ]
    }
   ],
   "source": [
    "lines = sc.textFile('file:///opt/code/SparkProgramming-PySpark/word.txt')\n",
    "pairRDD = lines.flatMap(lambda line: line.split(\" \")).map(lambda word: (word, 1))\n",
    "for p in pairRDD.collect():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5c738d",
   "metadata": {},
   "source": [
    "2.通过并行集合（列表）创建RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9da98818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Hadoop', 1)\n",
      "('Hive', 1)\n",
      "('Spark', 1)\n",
      "('Spark', 1)\n"
     ]
    }
   ],
   "source": [
    "list = [\"Hadoop\", \"Hive\", \"Spark\", \"Spark\"]\n",
    "rdd = sc.parallelize(list)\n",
    "pairRDD = rdd.map(lambda word: (word, 1))\n",
    "for p in pairRDD.collect():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d22667",
   "metadata": {},
   "source": [
    "### 常用的键值对转换操作\n",
    "\n",
    "常用的键值对转换操作包括 reduceByKey(func)、groupByKey()、keys、values、sortByKey()、mapValues(func)、join和combineByKey等\n",
    "\n",
    "- reduceByKey(func)\n",
    "\n",
    "reduceByKey(func)的功能是，使用func函数合并具有相同键的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0e876e66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hadoop', 1), ('Spark', 2), ('Hive', 1)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairRDD = sc.parallelize([(\"Hadoop\", 1), (\"Spark\", 1), (\"Hive\", 1), (\"Spark\", 1)])\n",
    "pairRDD.reduceByKey(lambda a,b: a+b).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c810d8",
   "metadata": {},
   "source": [
    "- groupByKey()\n",
    "groupByKey()的功能是，对具有相同键的值进行分组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c25d2e1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark', <pyspark.resultiterable.ResultIterable at 0x7fe9dc3a1940>),\n",
       " ('hadoop', <pyspark.resultiterable.ResultIterable at 0x7fe9dc3a1be0>)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list = [(\"spark\", 1), ('spark', 2), ('hadoop', 3), ('hadoop', 5)]\n",
    "pairRDD = sc.parallelize(list)\n",
    "pairRDD.groupByKey()\n",
    "pairRDD.groupByKey().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7aff4ab",
   "metadata": {},
   "source": [
    "**groupByKey和reduceByKey的区别**  \n",
    "- reduceByKey用于对每个key对应的多个value进行聚合操作，并且聚合操作可以通过函数func进行自定义\n",
    "- groupByKey也是对每个key进行操作，但是，对每个key只会生成一个value-list，groupByKey本身不能自定义函数，需要先用groupByKey生成RDD，然后才能对此RDD通过map进行自定义函数操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7aacabcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('one', 1), ('two', 1), ('three', 3)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = ['one', 'two', 'three', 'three', 'three']\n",
    "wordPairsRDD = sc.parallelize(words).map(lambda word: (word, 1))\n",
    "wordCountsWithReduce = wordPairsRDD.reduceByKey(lambda a,b: a+b)\n",
    "wordCountsWithReduce.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "92969a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('one', 1), ('two', 1), ('three', 3)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordCountsWithGroup = wordPairsRDD.groupByKey().map(lambda t: (t[0], sum(t[1])))\n",
    "wordCountsWithGroup.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9128ab5",
   "metadata": {},
   "source": [
    "- keys\n",
    "keys只会把PairRDD中key返回形成一个新的RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b23c3629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hadoop', 'spark', 'hive', 'spark']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list = [('hadoop', 1), ('spark', 1), ('hive', 1), ('spark', 1)]\n",
    "pairRDD = sc.parallelize(list)\n",
    "pairRDD.keys().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701f58ce",
   "metadata": {},
   "source": [
    "- values\n",
    "values只会把PairRDD中的value返回形成一个新的RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "920eda07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairRDD.values().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3989fcd1",
   "metadata": {},
   "source": [
    "- sortByKey()\n",
    "sortByKey()的功能返回一个根据键排序的RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b3b3a783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hadoop', 1), ('hive', 1), ('spark', 1), ('spark', 1)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairRDD.sortByKey().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28464944",
   "metadata": {},
   "source": [
    "- sortByKey()和sortBy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e5b6d5a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('d', 17), ('c', 25), ('b', 67), ('a', 42)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1 = sc.parallelize([('c', 8), ('b', 25), ('c', 17), ('a', 42), ('b', 42), ('d', 17)])\n",
    "d1.reduceByKey(lambda a,b: a+b).sortByKey(False).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d85a59fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('d', 17), ('c', 25), ('b', 67), ('a', 42)]\n",
      "[('d', 17), ('c', 25), ('b', 67), ('a', 42)]\n",
      "[('b', 67), ('a', 42), ('c', 25), ('d', 17)]\n"
     ]
    }
   ],
   "source": [
    "print(d1.reduceByKey(lambda a,b: a+b).sortBy(lambda x: x, False).collect())\n",
    "print(d1.reduceByKey(lambda a,b: a+b).sortBy(lambda x: x[0], False).collect())\n",
    "print(d1.reduceByKey(lambda a,b: a+b).sortBy(lambda x: x[1], False).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92846470",
   "metadata": {},
   "source": [
    "- mapValues(func)\n",
    "对键值对RDD中的每一个value都应用一个函数，但是，key不会发生变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b7833644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hadoop', 2), ('spark', 2), ('hive', 2), ('spark', 2)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list = [('hadoop', 1), ('spark', 1), ('hive', 1), ('spark', 1)]\n",
    "pairRDD = sc.parallelize(list)\n",
    "pairRDD1 = pairRDD.mapValues(lambda x: x+1)\n",
    "pairRDD1.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bfd826",
   "metadata": {},
   "source": [
    "- join  \n",
    "join()内连接，对于给定的两个输入数据集(K, V1)和(K, V2)，只有在两个数据集中都在存在的key才会被输出，最终得到一个(K, (V1, V2))的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4964b59d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark', (1, 'fast')), ('spark', (2, 'fast'))]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairRDD1 = sc.parallelize([('spark', 1), ('spark', 2), ('hadoop', 3), ('hadoop', 5)])\n",
    "pairRDD2 = sc.parallelize([('spark', 'fast')])\n",
    "pairRDD3 = pairRDD1.join(pairRDD2)\n",
    "pairRDD3.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb889b58",
   "metadata": {},
   "source": [
    "## 数据读写\n",
    "### 文件数据读写\n",
    "\n",
    "1.本地文件系统的数据读写  \n",
    "(1)从文件中读取数据创建RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7406a16d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hadoop is good'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile = sc.textFile('file:///opt/code/SparkProgramming-PySpark/word.txt')\n",
    "textFile.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc6ec1c",
   "metadata": {},
   "source": [
    "(2)把RDD写入到文本文件中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2190db51",
   "metadata": {},
   "outputs": [],
   "source": [
    "textFile.saveAsTextFile('file:///opt/code/SparkProgramming-PySpark/word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9fc769a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000  _SUCCESS\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33ad6b6",
   "metadata": {},
   "source": [
    "2.分布式文件系统HDFS的数据读写  \n",
    "从分布式文件系统HDFS中读取数据，也是采用textFile()方法，可以为textFile()方法提供一个HDFS文件或目录地址，如果是一个文件地址，它会加载该文件，如果是一个目录，则会加载该目录下的所有文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9e833824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'688316,青云科技-U'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile = sc.textFile('hdfs://localhost:9000/user/spark/stocks/part-m-00000')\n",
    "textFile.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b1df7900",
   "metadata": {},
   "outputs": [],
   "source": [
    "textFile.saveAsTextFile('writeback')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "35b5add7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "drwxr-xr-x   - spark supergroup          0 2021-12-09 03:44 /user/spark/stocks\r\n",
      "drwxr-xr-x   - spark supergroup          0 2021-12-10 20:13 /user/spark/writeback\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls /user/spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4256d0",
   "metadata": {},
   "source": [
    "### 读写HBase数据\n",
    "0.HBase简介  \n",
    "- HBase是一个高可靠、高性能、面向列、可伸缩的分布式数据库，主要用来存储非结构化和半结构化的松散数据。  \n",
    "- 每个值是一个未经解释的字符串，没有数据类型\n",
    "- 用户在表中存储数据，每一行都有一个可排序的行键和任意多的列\n",
    "- 表在水平方向由一个或多个列族组成，一个列族中可以包含任意多个列的数量以及类型，所有列均以字符串形式存储，用户需要自行进行数据类型转换\n",
    "- HBase中执行更新操作时，并不会删除数据旧的版本，而是生成一个新的版本\n",
    "\n",
    "\n",
    "- 表：HBase采用表来组织数据，表由行和列组成，列划分为若干列族\n",
    "- 行：每个HBase表都由若干行组成，每个行都由行键(row key)来标识\n",
    "- 列族：一个HBase表被分组成许多“列族”(Column Family)的集合，它是基本的访问控制单元\n",
    "- 列限定符：列族里的数据通过列限定符（或列）来定位\n",
    "- 单元格：在HBase表中，通过行、列族和列限定符确定一个“单元格”（cell），单元格中存储的数据没有数据类型，总被视为字符数组byte[]\n",
    "- 时间戳：每个单元格都保存着同一份数据的多个版本，这些版本采用时间戳进行索引\n",
    "\n",
    "1.[安装HBase](http://dblab.xmu.edu.cn/blog/install-hbase/)  \n",
    "\n",
    "- 创建一个HBase表\n",
    "```\n",
    "hbase> create 'student','info'\n",
    "hbase> put 'student','1','info:name','Xueqian'\n",
    "hbase> put 'student','1','info:gender','F'\n",
    "hbase> put 'student','1','info:age','23'\n",
    "hbase> put 'student','2','info:name','Weiliang'\n",
    "hbase> put 'student','2','info:gender','M'\n",
    "hbase> put 'student','2','info:age','25'\n",
    "```\n",
    "\n",
    "2.配置Spark  \n",
    "把HBase安装目录下的lib目录中的一些jar文件拷贝到Spark安装目录中，这些都是编程时需要用到的包。\n",
    "\n",
    "```shell\n",
    "$ cd $SPARK_HOME/jars\n",
    "$ mkdir hbase && cd hbase\n",
    "$ cp $HBASE_HOME/lib/hbase*.jar ./\n",
    "$ cp $HBASE_HOME/lib/guava* ./\n",
    "$ cp $HBASE_HOME/lib/htrace-core*.jar ./\n",
    "$ cp $HABSE_HOME/lib/protobuf*.jar ./\n",
    "```\n",
    "\n",
    "此外，在Spark2.0版本以上，缺少把HBase数据转换成Python可读取数据的jar包，需要另外下载。  \n",
    "https://mvnrepository.com/artifact/org.apache.spark/spark-examples_2.11/1.6.0-typesafe-001\n",
    "\n",
    "然后，打开spark-env.sh文件\n",
    "```shell\n",
    "export SPARK_DIST_CLASSPATH=$(/opt/software/hadoop/bin/hadoop classpath):$(/opt/software/hbase/bin/hbase classpath):/opt/software/spark/jars/hbase/*\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e76dcd2",
   "metadata": {},
   "source": [
    "3.编写程序读取HBase数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0274398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 {\"qualifier\" : \"age\", \"timestamp\" : \"1639187056536\", \"columnFamily\" : \"info\", \"row\" : \"1\", \"type\" : \"Put\", \"value\" : \"23\"}\n",
      "{\"qualifier\" : \"gender\", \"timestamp\" : \"1639187037733\", \"columnFamily\" : \"info\", \"row\" : \"1\", \"type\" : \"Put\", \"value\" : \"F\"}\n",
      "{\"qualifier\" : \"name\", \"timestamp\" : \"1639187002111\", \"columnFamily\" : \"info\", \"row\" : \"1\", \"type\" : \"Put\", \"value\" : \"Xueqian\"}\n",
      "2 {\"qualifier\" : \"age\", \"timestamp\" : \"1639187125950\", \"columnFamily\" : \"info\", \"row\" : \"2\", \"type\" : \"Put\", \"value\" : \"25\"}\n",
      "{\"qualifier\" : \"gender\", \"timestamp\" : \"1639187104846\", \"columnFamily\" : \"info\", \"row\" : \"2\", \"type\" : \"Put\", \"value\" : \"M\"}\n",
      "{\"qualifier\" : \"name\", \"timestamp\" : \"1639187078184\", \"columnFamily\" : \"info\", \"row\" : \"2\", \"type\" : \"Put\", \"value\" : \"Weiliang\"}\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster('local').setAppName('ReadHbase')\n",
    "sc = SparkContext(conf=conf)\n",
    "host = 'localhost'\n",
    "table = 'student'\n",
    "conf = {\"hbase.zookeeper.quorum\": host, \"hbase.mapreduce.inputtable\": table}\n",
    "keyConv = \"org.apache.spark.examples.pythonconverters.ImmutableBytesWritableToStringConverter\"\n",
    "valueConv = \"org.apache.spark.examples.pythonconverters.HBaseResultToStringConverter\"\n",
    "hbase_rdd = sc.newAPIHadoopRDD(\"org.apache.hadoop.hbase.mapreduce.TableInputFormat\",\n",
    "                               \"org.apache.hadoop.hbase.io.ImmutableBytesWritable\",\n",
    "                              \"org.apache.hadoop.hbase.client.Result\",\n",
    "                              keyConverter=keyConv, valueConverter=valueConv, conf=conf)\n",
    "count = hbase_rdd.count()\n",
    "hbase_rdd.cache()\n",
    "output = hbase_rdd.collect()\n",
    "for (k, v) in output:\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce882ad",
   "metadata": {},
   "source": [
    "当然，你可以通过spark-submit来提交任务\n",
    "\n",
    "在文件夹下创建SparkOperateHBase.py,复制粘贴以下代码\n",
    "```python\n",
    "#!/home/spark/miniconda3/envs/bigdata/bin/python\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster('local').setAppName('ReadHbase')\n",
    "sc = SparkContext(conf=conf)\n",
    "host = 'localhost'\n",
    "table = 'student'\n",
    "conf = {\"hbase.zookeeper.quorum\": host, \"hbase.mapreduce.inputtable\": table}\n",
    "keyConv = \"org.apache.spark.examples.pythonconverters.ImmutableBytesWritableToStringConverter\"\n",
    "valueConv = \"org.apache.spark.examples.pythonconverters.HBaseResultToStringConverter\"\n",
    "hbase_rdd = sc.newAPIHadoopRDD(\"org.apache.hadoop.hbase.mapreduce.TableInputFormat\",\n",
    "                               \"org.apache.hadoop.hbase.io.ImmutableBytesWritable\",\n",
    "                              \"org.apache.hadoop.hbase.client.Result\",\n",
    "                              keyConverter=keyConv, valueConverter=valueConv, conf=conf)\n",
    "count = hbase_rdd.count()\n",
    "hbase_rdd.cache()\n",
    "output = hbase_rdd.collect()\n",
    "for (k, v) in output:\n",
    "    print(k, v)\n",
    "```\n",
    "\n",
    "运行成功之后，你会看到\n",
    "```\n",
    "1 {\"qualifier\" : \"age\", \"timestamp\" : \"1639187056536\", \"columnFamily\" : \"info\", \"row\" : \"1\", \"type\" : \"Put\", \"value\" : \"23\"}\n",
    "{\"qualifier\" : \"gender\", \"timestamp\" : \"1639187037733\", \"columnFamily\" : \"info\", \"row\" : \"1\", \"type\" : \"Put\", \"value\" : \"F\"}\n",
    "{\"qualifier\" : \"name\", \"timestamp\" : \"1639187002111\", \"columnFamily\" : \"info\", \"row\" : \"1\", \"type\" : \"Put\", \"value\" : \"Xueqian\"}\n",
    "2 {\"qualifier\" : \"age\", \"timestamp\" : \"1639187125950\", \"columnFamily\" : \"info\", \"row\" : \"2\", \"type\" : \"Put\", \"value\" : \"25\"}\n",
    "{\"qualifier\" : \"gender\", \"timestamp\" : \"1639187104846\", \"columnFamily\" : \"info\", \"row\" : \"2\", \"type\" : \"Put\", \"value\" : \"M\"}\n",
    "{\"qualifier\" : \"name\", \"timestamp\" : \"1639187078184\", \"columnFamily\" : \"info\", \"row\" : \"2\", \"type\" : \"Put\", \"value\" : \"Weiliang\"}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c976e5c",
   "metadata": {},
   "source": [
    "4.编写程序向HBase写入数据库\n",
    "下面编写应用程序把表中的两个学生信息插入到HBase的student表中\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th rowspan=\"2\">id</th>\n",
    "    <th colspan=\"9\">info</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td colspan=\"3\"><center>name</center></td>\n",
    "    <td colspan=\"3\"><center>gender</center></td>\n",
    "    <td colspan=\"3\"><center>age</center></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td rowspan=\"1\"><center>3</center></td>\n",
    "    <td colspan=\"3\"><center>Rongcheng</center></td>\n",
    "    <td colspan=\"3\"><center>M</center></td>\n",
    "    <td colspan=\"3\"><center>26</center></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td rowspan=\"1\"><center>4</center></td>\n",
    "    <td colspan=\"3\"><center>Guanhua</center></td>\n",
    "    <td colspan=\"3\"><center>M</center></td>\n",
    "    <td colspan=\"3\"><center>27</center></td>\n",
    "  </tr>\n",
    "  </table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d49283b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"ReadHBase\")\n",
    "sc = SparkContext(conf = conf)\n",
    "host = 'localhost'\n",
    "table = 'student'\n",
    "keyConv = \"org.apache.spark.examples.pythonconverters.StringToImmutableBytesWritableConverter\"\n",
    "valueConv = \"org.apache.spark.examples.pythonconverters.StringListToPutConverter\"\n",
    "conf = {\"hbase.zookeeper.quorum\": host,\n",
    "        \"hbase.mapred.outputtable\": table,\n",
    "        \"mapreduce.outputformat.class\": \"org.apache.hadoop.hbase.mapreduce.TableOutputFormat\",\n",
    "        \"mapreduce.job.output.key.class\": \"org.apache.hadoop.hbase.io.ImmutableBytesWritable\",\n",
    "        \"mapreduce.job.output.value.class\": \"org.apache.hadoop.io.Writable\"}\n",
    "rawData = ['3,info,name,Rongcheng','3,info,gender,M','3,info,age,26',\n",
    "           '4,info,name,Guanhua','4,info,gender,M','4,info,age,27']\n",
    "sc.parallelize(rawData).map(lambda x: (x[0],x.split(',')))\\\n",
    "    .saveAsNewAPIHadoopDataset(conf=conf,keyConverter=keyConv,valueConverter=valueConv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb629f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/opt/software/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/opt/software/hbase/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "21/12/10 23:13:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "21/12/10 23:13:29 INFO spark.SparkContext: Running Spark version 2.4.2\n",
      "21/12/10 23:13:29 INFO spark.SparkContext: Submitted application: ReadHbase\n",
      "21/12/10 23:13:29 INFO spark.SecurityManager: Changing view acls to: spark\n",
      "21/12/10 23:13:29 INFO spark.SecurityManager: Changing modify acls to: spark\n",
      "21/12/10 23:13:29 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "21/12/10 23:13:29 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "21/12/10 23:13:29 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()\n",
      "21/12/10 23:13:29 INFO util.Utils: Successfully started service 'sparkDriver' on port 36089.\n",
      "21/12/10 23:13:29 INFO spark.SparkEnv: Registering MapOutputTracker\n",
      "21/12/10 23:13:30 INFO spark.SparkEnv: Registering BlockManagerMaster\n",
      "21/12/10 23:13:30 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "21/12/10 23:13:30 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "21/12/10 23:13:30 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-95559b78-1bc4-45c7-82f9-192b8c24e6e7\n",
      "21/12/10 23:13:30 INFO memory.MemoryStore: MemoryStore started with capacity 366.3 MB\n",
      "21/12/10 23:13:30 INFO spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "21/12/10 23:13:30 INFO util.log: Logging initialized @1264ms\n",
      "21/12/10 23:13:30 INFO server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown\n",
      "21/12/10 23:13:30 INFO server.Server: Started @1308ms\n",
      "21/12/10 23:13:30 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "21/12/10 23:13:30 WARN util.Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "21/12/10 23:13:30 INFO server.AbstractConnector: Started ServerConnector@6c36e6c8{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}\n",
      "21/12/10 23:13:30 INFO util.Utils: Successfully started service 'SparkUI' on port 4042.\n",
      "21/12/10 23:13:30 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1bc41e91{/jobs,null,AVAILABLE,@Spark}\n",
      "21/12/10 23:13:30 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1d64d1d4{/jobs/json,null,AVAILABLE,@Spark}\n",
      "21/12/10 23:13:30 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@20b32a1{/jobs/job,null,AVAILABLE,@Spark}\n",
      "21/12/10 23:13:30 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5c2a78b6{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "21/12/10 23:13:30 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@67fbf896{/stages,null,AVAILABLE,@Spark}\n",
      "21/12/10 23:13:30 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3c3e40f2{/stages/json,null,AVAILABLE,@Spark}\n",
      "21/12/10 23:13:30 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2917fdc4{/stages/stage,null,AVAILABLE,@Spark}\n",
      "21/12/10 23:13:30 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@50075664{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "21/12/10 23:13:30 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@46e8787d{/stages/pool,null,AVAILABLE,@Spark}\n",
      "21/12/10 23:13:30 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5e7b7437{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "21/12/10 23:13:30 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4574078c{/storage,null,AVAILABLE,@Spark}\n",
      "21/12/10 23:13:30 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2faacd30{/storage/json,null,AVAILABLE,@Spark}\n",
      "21/12/10 23:13:30 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2a847328{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "21/12/10 23:13:30 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f5b6083{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "21/12/10 23:13:30 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4af64a83{/environment,null,AVAILABLE,@Spark}\n",
      "21/12/10 23:13:30 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@334cdda0{/environment/json,null,AVAILABLE,@Spark}\n",
      "21/12/10 23:13:30 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3a58e5d2{/executors,null,AVAILABLE,@Spark}\n",
      "21/12/10 23:13:30 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4ef3d562{/executors/json,null,AVAILABLE,@Spark}\n",
      "21/12/10 23:13:30 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@36292c6d{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "21/12/10 23:13:30 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@635d56d5{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "21/12/10 23:13:30 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1e564919{/static,null,AVAILABLE,@Spark}\n",
      "21/12/10 23:13:30 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1107dcbd{/,null,AVAILABLE,@Spark}\n",
      "21/12/10 23:13:30 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6fd4cae9{/api,null,AVAILABLE,@Spark}\n",
      "21/12/10 23:13:30 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1bafe31e{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "21/12/10 23:13:30 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2e572c7a{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "21/12/10 23:13:30 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://alone:4042\n",
      "21/12/10 23:13:30 INFO executor.Executor: Starting executor ID driver on host localhost\n",
      "21/12/10 23:13:30 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42558.\n",
      "21/12/10 23:13:30 INFO netty.NettyBlockTransferService: Server created on alone:42558\n",
      "21/12/10 23:13:30 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "21/12/10 23:13:30 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, alone, 42558, None)\n",
      "21/12/10 23:13:30 INFO storage.BlockManagerMasterEndpoint: Registering block manager alone:42558 with 366.3 MB RAM, BlockManagerId(driver, alone, 42558, None)\n",
      "21/12/10 23:13:30 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, alone, 42558, None)\n",
      "21/12/10 23:13:30 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, alone, 42558, None)\n",
      "21/12/10 23:13:30 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@54feec9e{/metrics/json,null,AVAILABLE,@Spark}\n",
      "21/12/10 23:13:30 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 215.3 KB, free 366.1 MB)\n",
      "21/12/10 23:13:30 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.3 KB, free 366.1 MB)\n",
      "21/12/10 23:13:30 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on alone:42558 (size: 20.3 KB, free: 366.3 MB)\n",
      "21/12/10 23:13:30 INFO spark.SparkContext: Created broadcast 0 from newAPIHadoopRDD at PythonRDD.scala:318\n",
      "21/12/10 23:13:30 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 215.3 KB, free 365.9 MB)\n",
      "21/12/10 23:13:30 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 20.3 KB, free 365.8 MB)\n",
      "21/12/10 23:13:30 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on alone:42558 (size: 20.3 KB, free: 366.3 MB)\n",
      "21/12/10 23:13:30 INFO spark.SparkContext: Created broadcast 1 from broadcast at PythonRDD.scala:299\n",
      "21/12/10 23:13:30 INFO python.Converter: Loaded converter: org.apache.spark.examples.pythonconverters.ImmutableBytesWritableToStringConverter\n",
      "21/12/10 23:13:30 INFO python.Converter: Loaded converter: org.apache.spark.examples.pythonconverters.HBaseResultToStringConverter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/12/10 23:13:30 INFO zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x4de3729f connecting to ZooKeeper ensemble=localhost:2181\r\n",
      "21/12/10 23:13:30 INFO zookeeper.ZooKeeper: Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT\r\n",
      "21/12/10 23:13:30 INFO zookeeper.ZooKeeper: Client environment:host.name=alone\r\n",
      "21/12/10 23:13:30 INFO zookeeper.ZooKeeper: Client environment:java.version=1.8.0_162\r\n",
      "21/12/10 23:13:30 INFO zookeeper.ZooKeeper: Client environment:java.vendor=Oracle Corporation\r\n",
      "21/12/10 23:13:30 INFO zookeeper.ZooKeeper: Client environment:java.home=/opt/software/jdk/jre\r\n",
      "21/12/10 23:13:30 INFO zookeeper.ZooKeeper: Client environment:java.class.path=/opt/software/spark/conf/:/opt/software/spark/jars/commons-lang3-3.5.jar:/opt/software/spark/jars/arrow-format-0.10.0.jar:/opt/software/spark/jars/json4s-jackson_2.11-3.5.3.jar:/opt/software/spark/jars/jersey-container-servlet-2.22.2.jar:/opt/software/spark/jars/pyrolite-4.13.jar:/opt/software/spark/jars/hppc-0.7.2.jar:/opt/software/spark/jars/metrics-jvm-3.1.5.jar:/opt/software/spark/jars/jersey-guava-2.22.2.jar:/opt/software/spark/jars/commons-net-3.1.jar:/opt/software/spark/jars/validation-api-1.1.0.Final.jar:/opt/software/spark/jars/json4s-ast_2.11-3.5.3.jar:/opt/software/spark/jars/orc-mapreduce-1.5.5-nohive.jar:/opt/software/spark/jars/parquet-encoding-1.10.1.jar:/opt/software/spark/jars/spark-streaming_2.11-2.4.2.jar:/opt/software/spark/jars/spark-graphx_2.11-2.4.2.jar:/opt/software/spark/jars/jersey-container-servlet-core-2.22.2.jar:/opt/software/spark/jars/zjsonpatch-0.3.0.jar:/opt/software/spark/jars/scala-compiler-2.11.12.jar:/opt/software/spark/jars/jersey-media-jaxb-2.22.2.jar:/opt/software/spark/jars/avro-mapred-1.8.2-hadoop2.jar:/opt/software/spark/jars/antlr4-runtime-4.7.jar:/opt/software/spark/jars/jackson-module-jaxb-annotations-2.6.7.jar:/opt/software/spark/jars/okhttp-3.8.1.jar:/opt/software/spark/jars/leveldbjni-all-1.8.jar:/opt/software/spark/jars/spark-catalyst_2.11-2.4.2.jar:/opt/software/spark/jars/jackson-annotations-2.6.7.jar:/opt/software/spark/jars/spark-mesos_2.11-2.4.2.jar:/opt/software/spark/jars/jackson-module-paranamer-2.7.9.jar:/opt/software/spark/jars/jersey-server-2.22.2.jar:/opt/software/spark/jars/scala-parser-combinators_2.11-1.1.0.jar:/opt/software/spark/jars/spark-sketch_2.11-2.4.2.jar:/opt/software/spark/jars/arrow-memory-0.10.0.jar:/opt/software/spark/jars/hk2-utils-2.4.0-b34.jar:/opt/software/spark/jars/joda-time-2.9.3.jar:/opt/software/spark/jars/commons-crypto-1.0.0.jar:/opt/software/spark/jars/logging-interceptor-3.12.0.jar:/opt/software/spark/jars/parquet-common-1.10.1.jar:/opt/software/spark/jars/spark-yarn_2.11-2.4.2.jar:/opt/software/spark/jars/hk2-locator-2.4.0-b34.jar:/opt/software/spark/jars/chill-java-0.9.3.jar:/opt/software/spark/jars/opencsv-2.3.jar:/opt/software/spark/jars/lz4-java-1.4.0.jar:/opt/software/spark/jars/kubernetes-model-common-4.1.2.jar:/opt/software/spark/jars/breeze_2.11-0.13.2.jar:/opt/software/spark/jars/javax.inject-2.4.0-b34.jar:/opt/software/spark/jars/metrics-json-3.1.5.jar:/opt/software/spark/jars/parquet-column-1.10.1.jar:/opt/software/spark/jars/aopalliance-repackaged-2.4.0-b34.jar:/opt/software/spark/jars/jcl-over-slf4j-1.7.16.jar:/opt/software/spark/jars/parquet-jackson-1.10.1.jar:/opt/software/spark/jars/metrics-graphite-3.1.5.jar:/opt/software/spark/jars/spark-mllib_2.11-2.4.2.jar:/opt/software/spark/jars/netty-3.9.9.Final.jar:/opt/software/spark/jars/spark-kvstore_2.11-2.4.2.jar:/opt/software/spark/jars/RoaringBitmap-0.7.45.jar:/opt/software/spark/jars/compress-lzf-1.0.3.jar:/opt/software/spark/jars/jersey-common-2.22.2.jar:/opt/software/spark/jars/parquet-hadoop-1.10.1.jar:/opt/software/spark/jars/commons-lang-2.6.jar:/opt/software/spark/jars/orc-shims-1.5.5.jar:/opt/software/spark/jars/spark-repl_2.11-2.4.2.jar:/opt/software/spark/jars/jtransforms-2.4.0.jar:/opt/software/spark/jars/snakeyaml-1.15.jar:/opt/software/spark/jars/kubernetes-client-4.1.2.jar:/opt/software/spark/jars/snappy-java-1.1.7.3.jar:/opt/software/spark/jars/parquet-format-2.4.0.jar:/opt/software/spark/jars/kryo-shaded-4.0.2.jar:/opt/software/spark/jars/javax.ws.rs-api-2.0.1.jar:/opt/software/spark/jars/json4s-scalap_2.11-3.5.3.jar:/opt/software/spark/jars/spark-launcher_2.11-2.4.2.jar:/opt/software/spark/jars/paranamer-2.8.jar:/opt/software/spark/jars/janino-3.0.9.jar:/opt/software/spark/jars/xbean-asm6-shaded-4.8.jar:/opt/software/spark/jars/oro-2.0.8.jar:/opt/software/spark/jars/javax.annotation-api-1.2.jar:/opt/software/spark/jars/chill_2.11-0.9.3.jar:/opt/software/spark/jars/mesos-1.4.0-shaded-protobuf.jar:/opt/software/spark/jars/okio-1.13.0.jar:/opt/software/spark/jars/jackson-databind-2.6.7.1.jar:/opt/software/spark/jars/netty-all-4.1.17.Final.jar:/opt/software/spark/jars/commons-compiler-3.0.9.jar:/opt/software/spark/jars/commons-math3-3.4.1.jar:/opt/software/spark/jars/jul-to-slf4j-1.7.16.jar:/opt/software/spark/jars/core-1.1.2.jar:/opt/software/spark/jars/commons-codec-1.10.jar:/opt/software/spark/jars/zstd-jni-1.3.2-2.jar:/opt/software/spark/jars/univocity-parsers-2.7.3.jar:/opt/software/spark/jars/spark-unsafe_2.11-2.4.2.jar:/opt/software/spark/jars/jersey-client-2.22.2.jar:/opt/software/spark/jars/machinist_2.11-0.6.1.jar:/opt/software/spark/jars/shims-0.7.45.jar:/opt/software/spark/jars/spark-network-common_2.11-2.4.2.jar:/opt/software/spark/jars/osgi-resource-locator-1.0.1.jar:/opt/software/spark/jars/javassist-3.18.1-GA.jar:/opt/software/spark/jars/spire-macros_2.11-0.13.0.jar:/opt/software/spark/jars/spark-kubernetes_2.11-2.4.2.jar:/opt/software/spark/jars/spire_2.11-0.13.0.jar:/opt/software/spark/jars/jackson-core-2.6.7.jar:/opt/software/spark/jars/aircompressor-0.10.jar:/opt/software/spark/jars/scala-xml_2.11-1.0.5.jar:/opt/software/spark/jars/stream-2.7.0.jar:/opt/software/spark/jars/spark-network-shuffle_2.11-2.4.2.jar:/opt/software/spark/jars/macro-compat_2.11-1.1.1.jar:/opt/software/spark/jars/spark-mllib-local_2.11-2.4.2.jar:/opt/software/spark/jars/spark-tags_2.11-2.4.2-tests.jar:/opt/software/spark/jars/breeze-macros_2.11-0.13.2.jar:/opt/software/spark/jars/spark-sql_2.11-2.4.2.jar:/opt/software/spark/jars/shapeless_2.11-2.3.2.jar:/opt/software/spark/jars/minlog-1.3.0.jar:/opt/software/spark/jars/spark-tags_2.11-2.4.2.jar:/opt/software/spark/jars/kubernetes-model-4.1.2.jar:/opt/software/spark/jars/metrics-core-3.1.5.jar:/opt/software/spark/jars/jackson-module-scala_2.11-2.6.7.1.jar:/opt/software/spark/jars/automaton-1.11-8.jar:/opt/software/spark/jars/objenesis-2.5.1.jar:/opt/software/spark/jars/json4s-core_2.11-3.5.3.jar:/opt/software/spark/jars/py4j-0.10.7.jar:/opt/software/spark/jars/orc-core-1.5.5-nohive.jar:/opt/software/spark/jars/scala-reflect-2.11.12.jar:/opt/software/spark/jars/ivy-2.4.0.jar:/opt/software/spark/jars/scala-library-2.11.12.jar:/opt/software/spark/jars/spark-core_2.11-2.4.2.jar:/opt/software/spark/jars/jsr305-1.3.9.jar:/opt/software/spark/jars/arpack_combined_all-0.1.jar:/opt/software/spark/jars/hk2-api-2.4.0-b34.jar:/opt/software/spark/jars/jackson-dataformat-yaml-2.6.7.jar:/opt/software/spark/jars/arrow-vector-0.10.0.jar:/opt/software/spark/jars/flatbuffers-1.2.0-3f79e055.jar:/opt/software/spark/jars/generex-1.0.1.jar:/opt/software/spark/jars/javax.servlet-api-3.1.0.jar:/opt/software/hadoop/etc/hadoop/:/opt/software/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/software/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/software/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/software/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/software/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/opt/software/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/software/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/software/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/opt/software/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/software/hadoop/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/opt/software/hadoop/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/opt/software/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/software/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/opt/software/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/software/hadoop/share/hadoop/common/lib/asm-3.2.jar:/opt/software/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/software/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/opt/software/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/software/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/software/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/software/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/software/hadoop/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/opt/software/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/software/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/opt/software/hadoop/share/hadoop/common/lib/curator-client-2.6.0.jar:/opt/software/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/software/hadoop/share/hadoop/common/lib/curator-framework-2.6.0.jar:/opt/software/hadoop/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/opt/software/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/opt/software/hadoop/share/hadoop/common/lib/junit-4.11.jar:/opt/software/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/software/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/software/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/opt/software/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/software/hadoop/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/opt/software/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/opt/software/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/software/hadoop/share/hadoop/common/lib/htrace-core-3.0.4.jar:/opt/software/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/software/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/software/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/software/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/opt/software/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/software/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/opt/software/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/software/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/software/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/software/hadoop/share/hadoop/common/lib/jsr305-1.3.9.jar:/opt/software/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/software/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/software/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/opt/software/hadoop/share/hadoop/common/lib/commons-el-1.0.jar:/opt/software/hadoop/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/opt/software/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/opt/software/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/software/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/software/hadoop/share/hadoop/common/lib/xz-1.0.jar:/opt/software/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/software/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/software/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/software/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/software/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/opt/software/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/software/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/software/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/software/hadoop/share/hadoop/common/lib/activation-1.1.jar:/opt/software/hadoop/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/opt/software/hadoop/share/hadoop/common/hadoop-common-2.6.0.jar:/opt/software/hadoop/share/hadoop/common/hadoop-nfs-2.6.0.jar:/opt/software/hadoop/share/hadoop/hdfs/:/opt/software/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/software/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/software/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/software/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/software/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/software/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/software/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/software/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/software/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/software/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/software/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/software/hadoop/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/opt/software/hadoop/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/opt/software/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/software/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/software/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/software/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/software/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/software/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/software/hadoop/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/opt/software/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/software/hadoop/share/hadoop/hdfs/lib/commons-el-1.0.jar:/opt/software/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/software/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/software/hadoop/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/opt/software/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/software/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/opt/software/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/opt/software/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/opt/software/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/software/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/software/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/software/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/opt/software/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/software/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/software/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/opt/software/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/software/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/software/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/software/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/software/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/software/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/software/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/software/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/software/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/software/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/software/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/software/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/software/hadoop/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/opt/software/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/software/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/software/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/software/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/software/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/opt/software/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/software/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/software/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/software/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/software/hadoop/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/opt/software/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/software/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/software/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/software/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/opt/software/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/software/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/software/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/software/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/opt/software/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/opt/software/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/opt/software/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/opt/software/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/opt/software/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/opt/software/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/opt/software/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/opt/software/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/opt/software/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/opt/software/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/opt/software/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/opt/software/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/opt/software/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/software/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/software/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/software/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/opt/software/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/software/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/software/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/software/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/software/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/software/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/software/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/software/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/software/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/software/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/software/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/software/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/software/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/software/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/software/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/software/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/software/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/software/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/software/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/software/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/opt/software/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/opt/software/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/opt/software/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/opt/software/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/opt/software/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/opt/software/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/opt/software/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/opt/software/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/opt/software/hadoop/contrib/capacity-scheduler/*.jar:/opt/software/hbase/conf/:/opt/software/jdk/lib/tools.jar:/opt/software/hbase/:/opt/software/hbase/lib/activation-1.1.jar:/opt/software/hbase/lib/aopalliance-1.0.jar:/opt/software/hbase/lib/apacheds-i18n-2.0.0-M15.jar:/opt/software/hbase/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/software/hbase/lib/api-asn1-api-1.0.0-M20.jar:/opt/software/hbase/lib/api-util-1.0.0-M20.jar:/opt/software/hbase/lib/asm-3.1.jar:/opt/software/hbase/lib/avro-1.7.4.jar:/opt/software/hbase/lib/commons-beanutils-1.7.0.jar:/opt/software/hbase/lib/commons-beanutils-core-1.8.0.jar:/opt/software/hbase/lib/commons-cli-1.2.jar:/opt/software/hbase/lib/commons-codec-1.9.jar:/opt/software/hbase/lib/commons-collections-3.2.2.jar:/opt/software/hbase/lib/commons-compress-1.4.1.jar:/opt/software/hbase/lib/commons-configuration-1.6.jar:/opt/software/hbase/lib/commons-daemon-1.0.13.jar:/opt/software/hbase/lib/commons-digester-1.8.jar:/opt/software/hbase/lib/commons-el-1.0.jar:/opt/software/hbase/lib/commons-httpclient-3.1.jar:/opt/software/hbase/lib/commons-io-2.4.jar:/opt/software/hbase/lib/commons-lang-2.6.jar:/opt/software/hbase/lib/commons-logging-1.2.jar:/opt/software/hbase/lib/commons-math-2.2.jar:/opt/software/hbase/lib/commons-math3-3.1.1.jar:/opt/software/hbase/lib/commons-net-3.1.jar:/opt/software/hbase/lib/disruptor-3.3.0.jar:/opt/software/hbase/lib/findbugs-annotations-1.3.9-1.jar:/opt/software/hbase/lib/guava-12.0.1.jar:/opt/software/hbase/lib/guice-3.0.jar:/opt/software/hbase/lib/guice-servlet-3.0.jar:/opt/software/hbase/lib/hadoop-annotations-2.5.1.jar:/opt/software/hbase/lib/hadoop-auth-2.5.1.jar:/opt/software/hbase/lib/hadoop-client-2.5.1.jar:/opt/software/hbase/lib/hadoop-common-2.5.1.jar:/opt/software/hbase/lib/hadoop-hdfs-2.5.1.jar:/opt/software/hbase/lib/hadoop-mapreduce-client-app-2.5.1.jar:/opt/software/hbase/lib/hadoop-mapreduce-client-common-2.5.1.jar:/opt/software/hbase/lib/hadoop-mapreduce-client-core-2.5.1.jar:/opt/software/hbase/lib/hadoop-mapreduce-client-jobclient-2.5.1.jar:/opt/software/hbase/lib/hadoop-mapreduce-client-shuffle-2.5.1.jar:/opt/software/hbase/lib/hadoop-yarn-api-2.5.1.jar:/opt/software/hbase/lib/hadoop-yarn-client-2.5.1.jar:/opt/software/hbase/lib/hadoop-yarn-common-2.5.1.jar:/opt/software/hbase/lib/hadoop-yarn-server-common-2.5.1.jar:/opt/software/hbase/lib/hbase-annotations-1.2.4.jar:/opt/software/hbase/lib/hbase-annotations-1.2.4-tests.jar:/opt/software/hbase/lib/hbase-client-1.2.4.jar:/opt/software/hbase/lib/hbase-common-1.2.4.jar:/opt/software/hbase/lib/hbase-common-1.2.4-tests.jar:/opt/software/hbase/lib/hbase-examples-1.2.4.jar:/opt/software/hbase/lib/hbase-external-blockcache-1.2.4.jar:/opt/software/hbase/lib/hbase-hadoop2-compat-1.2.4.jar:/opt/software/hbase/lib/hbase-hadoop-compat-1.2.4.jar:/opt/software/hbase/lib/hbase-it-1.2.4.jar:/opt/software/hbase/lib/hbase-it-1.2.4-tests.jar:/opt/software/hbase/lib/hbase-prefix-tree-1.2.4.jar:/opt/software/hbase/lib/hbase-procedure-1.2.4.jar:/opt/software/hbase/lib/hbase-protocol-1.2.4.jar:/opt/software/hbase/lib/hbase-resource-bundle-1.2.4.jar:/opt/software/hbase/lib/hbase-rest-1.2.4.jar:/opt/software/hbase/lib/hbase-server-1.2.4.jar:/opt/software/hbase/lib/hbase-server-1.2.4-tests.jar:/opt/software/hbase/lib/hbase-shell-1.2.4.jar:/opt/software/hbase/lib/hbase-thrift-1.2.4.jar:/opt/software/hbase/lib/htrace-core-3.1.0-incubating.jar:/opt/software/hbase/lib/httpclient-4.2.5.jar:/opt/software/hbase/lib/httpcore-4.4.1.jar:/opt/software/hbase/lib/jackson-core-asl-1.9.13.jar:/opt/software/hbase/lib/jackson-jaxrs-1.9.13.jar:/opt/software/hbase/lib/jackson-mapper-asl-1.9.13.jar:/opt/software/hbase/lib/jackson-xc-1.9.13.jar:/opt/software/hbase/lib/jamon-runtime-2.4.1.jar:/opt/software/hbase/lib/jasper-compiler-5.5.23.jar:/opt/software/hbase/lib/jasper-runtime-5.5.23.jar:/opt/software/hbase/lib/javax.inject-1.jar:/opt/software/hbase/lib/java-xmlbuilder-0.4.jar:/opt/software/hbase/lib/jaxb-api-2.2.2.jar:/opt/software/hbase/lib/jaxb-impl-2.2.3-1.jar:/opt/software/hbase/lib/jcodings-1.0.8.jar:/opt/software/hbase/lib/jersey-client-1.9.jar:/opt/software/hbase/lib/jersey-core-1.9.jar:/opt/software/hbase/lib/jersey-guice-1.9.jar:/opt/software/hbase/lib/jersey-json-1.9.jar:/opt/software/hbase/lib/jersey-server-1.9.jar:/opt/software/hbase/lib/jets3t-0.9.0.jar:/opt/software/hbase/lib/jettison-1.3.3.jar:/opt/software/hbase/lib/jetty-6.1.26.jar:/opt/software/hbase/lib/jetty-sslengine-6.1.26.jar:/opt/software/hbase/lib/jetty-util-6.1.26.jar:/opt/software/hbase/lib/joni-2.1.2.jar:/opt/software/hbase/lib/jruby-complete-1.6.8.jar:/opt/software/hbase/lib/jsch-0.1.42.jar:/opt/software/hbase/lib/jsp-2.1-6.1.14.jar:/opt/software/hbase/lib/jsp-api-2.1-6.1.14.jar:/opt/software/hbase/lib/junit-4.12.jar:/opt/software/hbase/lib/leveldbjni-all-1.8.jar:/opt/software/hbase/lib/libthrift-0.9.3.jar:/opt/software/hbase/lib/log4j-1.2.17.jar:/opt/software/hbase/lib/metrics-core-2.2.0.jar:/opt/software/hbase/lib/netty-all-4.0.23.Final.jar:/opt/software/hbase/lib/paranamer-2.3.jar:/opt/software/hbase/lib/protobuf-java-2.5.0.jar:/opt/software/hbase/lib/servlet-api-2.5-6.1.14.jar:/opt/software/hbase/lib/servlet-api-2.5.jar:/opt/software/hbase/lib/slf4j-api-1.7.7.jar:/opt/software/hbase/lib/slf4j-log4j12-1.7.5.jar:/opt/software/hbase/lib/snappy-java-1.0.4.1.jar:/opt/software/hbase/lib/spymemcached-2.11.6.jar:/opt/software/hbase/lib/xmlenc-0.52.jar:/opt/software/hbase/lib/xz-1.0.jar:/opt/software/hbase/lib/zookeeper-3.4.6.jar:/opt/software/hadoop/conf:/opt/software/spark/jars/hbase/hbase-annotations-1.2.4.jar:/opt/software/spark/jars/hbase/hbase-annotations-1.2.4-tests.jar:/opt/software/spark/jars/hbase/hbase-client-1.2.4.jar:/opt/software/spark/jars/hbase/hbase-common-1.2.4.jar:/opt/software/spark/jars/hbase/hbase-common-1.2.4-tests.jar:/opt/software/spark/jars/hbase/hbase-examples-1.2.4.jar:/opt/software/spark/jars/hbase/hbase-external-blockcache-1.2.4.jar:/opt/software/spark/jars/hbase/hbase-hadoop2-compat-1.2.4.jar:/opt/software/spark/jars/hbase/hbase-hadoop-compat-1.2.4.jar:/opt/software/spark/jars/hbase/hbase-it-1.2.4.jar:/opt/software/spark/jars/hbase/hbase-it-1.2.4-tests.jar:/opt/software/spark/jars/hbase/hbase-prefix-tree-1.2.4.jar:/opt/software/spark/jars/hbase/hbase-procedure-1.2.4.jar:/opt/software/spark/jars/hbase/hbase-protocol-1.2.4.jar:/opt/software/spark/jars/hbase/hbase-resource-bundle-1.2.4.jar:/opt/software/spark/jars/hbase/hbase-rest-1.2.4.jar:/opt/software/spark/jars/hbase/hbase-server-1.2.4.jar:/opt/software/spark/jars/hbase/hbase-server-1.2.4-tests.jar:/opt/software/spark/jars/hbase/hbase-shell-1.2.4.jar:/opt/software/spark/jars/hbase/hbase-thrift-1.2.4.jar:/opt/software/spark/jars/hbase/guava-12.0.1.jar:/opt/software/spark/jars/hbase/htrace-core-3.1.0-incubating.jar:/opt/software/spark/jars/hbase/protobuf-java-2.5.0.jar:/opt/software/spark/jars/hbase/spark-examples_2.11-1.6.0-typesafe-001.jar\r\n",
      "21/12/10 23:13:30 INFO zookeeper.ZooKeeper: Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib\r\n",
      "21/12/10 23:13:30 INFO zookeeper.ZooKeeper: Client environment:java.io.tmpdir=/tmp\r\n",
      "21/12/10 23:13:30 INFO zookeeper.ZooKeeper: Client environment:java.compiler=<NA>\r\n",
      "21/12/10 23:13:30 INFO zookeeper.ZooKeeper: Client environment:os.name=Linux\r\n",
      "21/12/10 23:13:30 INFO zookeeper.ZooKeeper: Client environment:os.arch=amd64\r\n",
      "21/12/10 23:13:30 INFO zookeeper.ZooKeeper: Client environment:os.version=3.10.0-1160.el7.x86_64\r\n",
      "21/12/10 23:13:30 INFO zookeeper.ZooKeeper: Client environment:user.name=spark\r\n",
      "21/12/10 23:13:30 INFO zookeeper.ZooKeeper: Client environment:user.home=/home/spark\r\n",
      "21/12/10 23:13:30 INFO zookeeper.ZooKeeper: Client environment:user.dir=/opt/code/SparkProgramming-PySpark\r\n",
      "21/12/10 23:13:30 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=localhost:2181 sessionTimeout=180000 watcher=hconnection-0x4de3729f0x0, quorum=localhost:2181, baseZNode=/hbase\r\n",
      "21/12/10 23:13:30 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)\r\n",
      "21/12/10 23:13:30 INFO zookeeper.ClientCnxn: Socket connection established to localhost/0:0:0:0:0:0:0:1:2181, initiating session\r\n",
      "21/12/10 23:13:30 INFO zookeeper.ClientCnxn: Session establishment complete on server localhost/0:0:0:0:0:0:0:1:2181, sessionid = 0x17da40c8726001a, negotiated timeout = 90000\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/12/10 23:13:31 INFO util.RegionSizeCalculator: Calculating region sizes for table \"student\".\n",
      "21/12/10 23:13:31 INFO client.ConnectionManager$HConnectionImplementation: Closing master protocol: MasterService\n",
      "21/12/10 23:13:31 INFO client.ConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x17da40c8726001a\n",
      "21/12/10 23:13:31 INFO zookeeper.ZooKeeper: Session: 0x17da40c8726001a closed\n",
      "21/12/10 23:13:31 INFO zookeeper.ClientCnxn: EventThread shut down\n",
      "21/12/10 23:13:31 INFO spark.SparkContext: Starting job: take at SerDeUtil.scala:239\n",
      "21/12/10 23:13:31 INFO scheduler.DAGScheduler: Got job 0 (take at SerDeUtil.scala:239) with 1 output partitions\n",
      "21/12/10 23:13:31 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (take at SerDeUtil.scala:239)\n",
      "21/12/10 23:13:31 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "21/12/10 23:13:31 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "21/12/10 23:13:31 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at PythonHadoopUtil.scala:181), which has no missing parents\n",
      "21/12/10 23:13:31 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 3.1 KB, free 365.8 MB)\n",
      "21/12/10 23:13:31 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 1942.0 B, free 365.8 MB)\n",
      "21/12/10 23:13:31 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on alone:42558 (size: 1942.0 B, free: 366.3 MB)\n",
      "21/12/10 23:13:31 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1161\n",
      "21/12/10 23:13:31 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at PythonHadoopUtil.scala:181) (first 15 tasks are for partitions Vector(0))\n",
      "21/12/10 23:13:31 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 1 tasks\n",
      "21/12/10 23:13:31 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 7898 bytes)\n",
      "21/12/10 23:13:31 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "21/12/10 23:13:31 INFO rdd.NewHadoopRDD: Input split: HBase table split(table name: student, scan: , start row: , end row: , region location: alone)\n",
      "21/12/10 23:13:31 INFO zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x3b938b7d connecting to ZooKeeper ensemble=localhost:2181\n",
      "21/12/10 23:13:31 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=localhost:2181 sessionTimeout=180000 watcher=hconnection-0x3b938b7d0x0, quorum=localhost:2181, baseZNode=/hbase\n",
      "21/12/10 23:13:31 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "21/12/10 23:13:31 INFO zookeeper.ClientCnxn: Socket connection established to localhost/0:0:0:0:0:0:0:1:2181, initiating session\n",
      "21/12/10 23:13:31 INFO zookeeper.ClientCnxn: Session establishment complete on server localhost/0:0:0:0:0:0:0:1:2181, sessionid = 0x17da40c8726001b, negotiated timeout = 90000\n",
      "21/12/10 23:13:31 INFO mapreduce.TableInputFormatBase: Input split length: 0 bytes.\n",
      "21/12/10 23:13:31 INFO client.ConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x17da40c8726001b\n",
      "21/12/10 23:13:31 INFO zookeeper.ZooKeeper: Session: 0x17da40c8726001b closed\n",
      "21/12/10 23:13:31 INFO zookeeper.ClientCnxn: EventThread shut down\n",
      "21/12/10 23:13:31 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 1198 bytes result sent to driver\n",
      "21/12/10 23:13:31 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 105 ms on localhost (executor driver) (1/1)\n",
      "21/12/10 23:13:31 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "21/12/10 23:13:31 INFO scheduler.DAGScheduler: ResultStage 0 (take at SerDeUtil.scala:239) finished in 0.140 s\n",
      "21/12/10 23:13:31 INFO scheduler.DAGScheduler: Job 0 finished: take at SerDeUtil.scala:239, took 0.166477 s\n",
      "21/12/10 23:13:31 INFO spark.SparkContext: Starting job: count at /home/spark/pyspark-pro/SparkOperateHBase.py:15\n",
      "21/12/10 23:13:31 INFO scheduler.DAGScheduler: Got job 1 (count at /home/spark/pyspark-pro/SparkOperateHBase.py:15) with 1 output partitions\n",
      "21/12/10 23:13:31 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (count at /home/spark/pyspark-pro/SparkOperateHBase.py:15)\n",
      "21/12/10 23:13:31 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "21/12/10 23:13:31 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "21/12/10 23:13:31 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (PythonRDD[3] at count at /home/spark/pyspark-pro/SparkOperateHBase.py:15), which has no missing parents\n",
      "21/12/10 23:13:31 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 7.4 KB, free 365.8 MB)\n",
      "21/12/10 23:13:31 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.7 KB, free 365.8 MB)\n",
      "21/12/10 23:13:31 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on alone:42558 (size: 4.7 KB, free: 366.3 MB)\n",
      "21/12/10 23:13:31 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1161\n",
      "21/12/10 23:13:31 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (PythonRDD[3] at count at /home/spark/pyspark-pro/SparkOperateHBase.py:15) (first 15 tasks are for partitions Vector(0))\n",
      "21/12/10 23:13:31 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0 with 1 tasks\n",
      "21/12/10 23:13:31 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 7898 bytes)\n",
      "21/12/10 23:13:31 INFO executor.Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
      "21/12/10 23:13:31 INFO rdd.NewHadoopRDD: Input split: HBase table split(table name: student, scan: , start row: , end row: , region location: alone)\n",
      "21/12/10 23:13:31 INFO zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x7bc468dd connecting to ZooKeeper ensemble=localhost:2181\n",
      "21/12/10 23:13:31 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=localhost:2181 sessionTimeout=180000 watcher=hconnection-0x7bc468dd0x0, quorum=localhost:2181, baseZNode=/hbase\n",
      "21/12/10 23:13:31 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "21/12/10 23:13:31 INFO zookeeper.ClientCnxn: Socket connection established to localhost/0:0:0:0:0:0:0:1:2181, initiating session\n",
      "21/12/10 23:13:31 INFO zookeeper.ClientCnxn: Session establishment complete on server localhost/0:0:0:0:0:0:0:1:2181, sessionid = 0x17da40c8726001c, negotiated timeout = 90000\n",
      "21/12/10 23:13:31 INFO mapreduce.TableInputFormatBase: Input split length: 0 bytes.\n",
      "21/12/10 23:13:31 INFO client.ConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x17da40c8726001c\n",
      "21/12/10 23:13:31 INFO zookeeper.ZooKeeper: Session: 0x17da40c8726001c closed\n",
      "21/12/10 23:13:31 INFO zookeeper.ClientCnxn: EventThread shut down\n",
      "21/12/10 23:13:31 INFO python.PythonRunner: Times: total = 288, boot = 241, init = 46, finish = 1\n",
      "21/12/10 23:13:31 INFO executor.Executor: Finished task 0.0 in stage 1.0 (TID 1). 1504 bytes result sent to driver\n",
      "21/12/10 23:13:31 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 315 ms on localhost (executor driver) (1/1)\n",
      "21/12/10 23:13:31 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "21/12/10 23:13:31 INFO python.PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 55063\n",
      "21/12/10 23:13:31 INFO scheduler.DAGScheduler: ResultStage 1 (count at /home/spark/pyspark-pro/SparkOperateHBase.py:15) finished in 0.323 s\n",
      "21/12/10 23:13:31 INFO scheduler.DAGScheduler: Job 1 finished: count at /home/spark/pyspark-pro/SparkOperateHBase.py:15, took 0.326116 s\n",
      "21/12/10 23:13:31 INFO spark.SparkContext: Starting job: collect at /home/spark/pyspark-pro/SparkOperateHBase.py:17\n",
      "21/12/10 23:13:31 INFO scheduler.DAGScheduler: Got job 2 (collect at /home/spark/pyspark-pro/SparkOperateHBase.py:17) with 1 output partitions\n",
      "21/12/10 23:13:31 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (collect at /home/spark/pyspark-pro/SparkOperateHBase.py:17)\n",
      "21/12/10 23:13:31 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "21/12/10 23:13:31 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "21/12/10 23:13:31 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[2] at mapPartitions at SerDeUtil.scala:244), which has no missing parents\n",
      "21/12/10 23:13:31 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 3.4 KB, free 365.8 MB)\n",
      "21/12/10 23:13:31 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.0 KB, free 365.8 MB)\n",
      "21/12/10 23:13:31 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on alone:42558 (size: 2.0 KB, free: 366.3 MB)\n",
      "21/12/10 23:13:31 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1161\n",
      "21/12/10 23:13:31 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[2] at mapPartitions at SerDeUtil.scala:244) (first 15 tasks are for partitions Vector(0))\n",
      "21/12/10 23:13:31 INFO scheduler.TaskSchedulerImpl: Adding task set 2.0 with 1 tasks\n",
      "21/12/10 23:13:31 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 7898 bytes)\n",
      "21/12/10 23:13:31 INFO executor.Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
      "21/12/10 23:13:31 INFO rdd.NewHadoopRDD: Input split: HBase table split(table name: student, scan: , start row: , end row: , region location: alone)\n",
      "21/12/10 23:13:31 INFO zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x1bcac1ad connecting to ZooKeeper ensemble=localhost:2181\n",
      "21/12/10 23:13:31 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=localhost:2181 sessionTimeout=180000 watcher=hconnection-0x1bcac1ad0x0, quorum=localhost:2181, baseZNode=/hbase\n",
      "21/12/10 23:13:31 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "21/12/10 23:13:31 INFO zookeeper.ClientCnxn: Socket connection established to localhost/0:0:0:0:0:0:0:1:2181, initiating session\n",
      "21/12/10 23:13:31 INFO zookeeper.ClientCnxn: Session establishment complete on server localhost/0:0:0:0:0:0:0:1:2181, sessionid = 0x17da40c8726001d, negotiated timeout = 90000\n",
      "21/12/10 23:13:31 INFO mapreduce.TableInputFormatBase: Input split length: 0 bytes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/12/10 23:13:31 INFO spark.ContextCleaner: Cleaned accumulator 24\n",
      "21/12/10 23:13:31 INFO spark.ContextCleaner: Cleaned accumulator 41\n",
      "21/12/10 23:13:31 INFO spark.ContextCleaner: Cleaned accumulator 8\n",
      "21/12/10 23:13:31 INFO spark.ContextCleaner: Cleaned accumulator 44\n",
      "21/12/10 23:13:31 INFO spark.ContextCleaner: Cleaned accumulator 7\n",
      "21/12/10 23:13:31 INFO spark.ContextCleaner: Cleaned accumulator 18\n",
      "21/12/10 23:13:31 INFO spark.ContextCleaner: Cleaned accumulator 23\n",
      "21/12/10 23:13:31 INFO spark.ContextCleaner: Cleaned accumulator 13\n",
      "21/12/10 23:13:31 INFO spark.ContextCleaner: Cleaned accumulator 4\n",
      "21/12/10 23:13:31 INFO spark.ContextCleaner: Cleaned accumulator 42\n",
      "21/12/10 23:13:31 INFO spark.ContextCleaner: Cleaned accumulator 40\n",
      "21/12/10 23:13:31 INFO spark.ContextCleaner: Cleaned accumulator 46\n",
      "21/12/10 23:13:31 INFO spark.ContextCleaner: Cleaned accumulator 32\n",
      "21/12/10 23:13:31 INFO spark.ContextCleaner: Cleaned accumulator 36\n",
      "21/12/10 23:13:31 INFO spark.ContextCleaner: Cleaned accumulator 45\n",
      "21/12/10 23:13:31 INFO spark.ContextCleaner: Cleaned accumulator 47\n",
      "21/12/10 23:13:31 INFO spark.ContextCleaner: Cleaned accumulator 50\n",
      "21/12/10 23:13:31 INFO spark.ContextCleaner: Cleaned accumulator 16\n",
      "21/12/10 23:13:31 INFO spark.ContextCleaner: Cleaned accumulator 30\n",
      "21/12/10 23:13:31 INFO spark.ContextCleaner: Cleaned accumulator 17\n",
      "21/12/10 23:13:31 INFO spark.ContextCleaner: Cleaned accumulator 14\n",
      "21/12/10 23:13:31 INFO spark.ContextCleaner: Cleaned accumulator 10\n",
      "21/12/10 23:13:31 INFO spark.ContextCleaner: Cleaned accumulator 20\n",
      "21/12/10 23:13:31 INFO spark.ContextCleaner: Cleaned accumulator 11\n",
      "21/12/10 23:13:31 INFO spark.ContextCleaner: Cleaned accumulator 15\n",
      "21/12/10 23:13:31 INFO spark.ContextCleaner: Cleaned accumulator 34\n",
      "21/12/10 23:13:31 INFO spark.ContextCleaner: Cleaned accumulator 28\n",
      "21/12/10 23:13:31 INFO spark.ContextCleaner: Cleaned accumulator 2\n",
      "21/12/10 23:13:31 INFO spark.ContextCleaner: Cleaned accumulator 9\n",
      "21/12/10 23:13:31 INFO spark.ContextCleaner: Cleaned accumulator 27\n",
      "21/12/10 23:13:31 INFO spark.ContextCleaner: Cleaned accumulator 43\n",
      "21/12/10 23:13:31 INFO spark.ContextCleaner: Cleaned accumulator 21\n",
      "21/12/10 23:13:31 INFO spark.ContextCleaner: Cleaned accumulator 6\n",
      "21/12/10 23:13:31 INFO spark.ContextCleaner: Cleaned accumulator 39\n",
      "21/12/10 23:13:31 INFO spark.ContextCleaner: Cleaned accumulator 37\n",
      "21/12/10 23:13:31 INFO spark.ContextCleaner: Cleaned accumulator 12\n",
      "21/12/10 23:13:31 INFO spark.ContextCleaner: Cleaned accumulator 31\n",
      "21/12/10 23:13:31 INFO spark.ContextCleaner: Cleaned accumulator 26\n",
      "21/12/10 23:13:31 INFO spark.ContextCleaner: Cleaned accumulator 29\n",
      "21/12/10 23:13:31 INFO spark.ContextCleaner: Cleaned accumulator 22\n",
      "21/12/10 23:13:31 INFO spark.ContextCleaner: Cleaned accumulator 35\n",
      "21/12/10 23:13:31 INFO spark.ContextCleaner: Cleaned accumulator 3\n",
      "21/12/10 23:13:31 INFO spark.ContextCleaner: Cleaned accumulator 38\n",
      "21/12/10 23:13:31 INFO spark.ContextCleaner: Cleaned accumulator 49\n",
      "21/12/10 23:13:31 INFO spark.ContextCleaner: Cleaned accumulator 25\n",
      "21/12/10 23:13:31 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on alone:42558 in memory (size: 4.7 KB, free: 366.3 MB)\n",
      "21/12/10 23:13:31 INFO spark.ContextCleaner: Cleaned accumulator 19\n",
      "21/12/10 23:13:31 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on alone:42558 in memory (size: 1942.0 B, free: 366.3 MB)\n",
      "21/12/10 23:13:31 INFO spark.ContextCleaner: Cleaned accumulator 33\n",
      "21/12/10 23:13:31 INFO spark.ContextCleaner: Cleaned accumulator 48\n",
      "21/12/10 23:13:31 INFO spark.ContextCleaner: Cleaned accumulator 1\n",
      "21/12/10 23:13:31 INFO spark.ContextCleaner: Cleaned accumulator 5\n",
      "21/12/10 23:13:31 INFO client.ConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x17da40c8726001d\n",
      "21/12/10 23:13:31 INFO zookeeper.ZooKeeper: Session: 0x17da40c8726001d closed\n",
      "21/12/10 23:13:31 INFO zookeeper.ClientCnxn: EventThread shut down\n",
      "21/12/10 23:13:31 INFO memory.MemoryStore: Block rdd_2_0 stored as values in memory (estimated size 459.0 B, free 365.8 MB)\n",
      "21/12/10 23:13:31 INFO storage.BlockManagerInfo: Added rdd_2_0 in memory on alone:42558 (size: 459.0 B, free: 366.3 MB)\n",
      "21/12/10 23:13:32 INFO executor.Executor: Finished task 0.0 in stage 2.0 (TID 2). 2426 bytes result sent to driver\n",
      "21/12/10 23:13:32 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 167 ms on localhost (executor driver) (1/1)\n",
      "21/12/10 23:13:32 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "21/12/10 23:13:32 INFO scheduler.DAGScheduler: ResultStage 2 (collect at /home/spark/pyspark-pro/SparkOperateHBase.py:17) finished in 0.172 s\n",
      "21/12/10 23:13:32 INFO scheduler.DAGScheduler: Job 2 finished: collect at /home/spark/pyspark-pro/SparkOperateHBase.py:17, took 0.177423 s\n",
      "1 {\"qualifier\" : \"age\", \"timestamp\" : \"1639187056536\", \"columnFamily\" : \"info\", \"row\" : \"1\", \"type\" : \"Put\", \"value\" : \"23\"}\n",
      "{\"qualifier\" : \"gender\", \"timestamp\" : \"1639187037733\", \"columnFamily\" : \"info\", \"row\" : \"1\", \"type\" : \"Put\", \"value\" : \"F\"}\n",
      "{\"qualifier\" : \"name\", \"timestamp\" : \"1639187002111\", \"columnFamily\" : \"info\", \"row\" : \"1\", \"type\" : \"Put\", \"value\" : \"Xueqian\"}\n",
      "2 {\"qualifier\" : \"age\", \"timestamp\" : \"1639187125950\", \"columnFamily\" : \"info\", \"row\" : \"2\", \"type\" : \"Put\", \"value\" : \"25\"}\n",
      "{\"qualifier\" : \"gender\", \"timestamp\" : \"1639187104846\", \"columnFamily\" : \"info\", \"row\" : \"2\", \"type\" : \"Put\", \"value\" : \"M\"}\n",
      "{\"qualifier\" : \"name\", \"timestamp\" : \"1639187078184\", \"columnFamily\" : \"info\", \"row\" : \"2\", \"type\" : \"Put\", \"value\" : \"Weiliang\"}\n",
      "3 {\"qualifier\" : \"age\", \"timestamp\" : \"1639206679457\", \"columnFamily\" : \"info\", \"row\" : \"3\", \"type\" : \"Put\", \"value\" : \"26\"}\n",
      "{\"qualifier\" : \"gender\", \"timestamp\" : \"1639206679457\", \"columnFamily\" : \"info\", \"row\" : \"3\", \"type\" : \"Put\", \"value\" : \"M\"}\n",
      "{\"qualifier\" : \"name\", \"timestamp\" : \"1639206679457\", \"columnFamily\" : \"info\", \"row\" : \"3\", \"type\" : \"Put\", \"value\" : \"Rongcheng\"}\n",
      "4 {\"qualifier\" : \"age\", \"timestamp\" : \"1639206679457\", \"columnFamily\" : \"info\", \"row\" : \"4\", \"type\" : \"Put\", \"value\" : \"27\"}\n",
      "{\"qualifier\" : \"gender\", \"timestamp\" : \"1639206679457\", \"columnFamily\" : \"info\", \"row\" : \"4\", \"type\" : \"Put\", \"value\" : \"M\"}\n",
      "{\"qualifier\" : \"name\", \"timestamp\" : \"1639206679457\", \"columnFamily\" : \"info\", \":\u001b[K21/12/10 23:13:32 INFO spark.SparkContext: Invoking stop() from shutdown hook\n",
      "21/12/10 23:13:32 INFO server.AbstractConnector: Stopped Spark@6c36e6c8{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}\n",
      "21/12/10 23:13:32 INFO ui.SparkUI: Stopped Spark web UI at http://alone:4042\n",
      "21/12/10 23:13:32 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "21/12/10 23:13:32 INFO memory.MemoryStore: MemoryStore cleared\n",
      "21/12/10 23:13:32 INFO storage.BlockManager: BlockManager stopped\n",
      "21/12/10 23:13:32 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\n",
      "21/12/10 23:13:32 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "21/12/10 23:13:32 INFO spark.SparkContext: Successfully stopped SparkContext\n",
      "21/12/10 23:13:32 INFO util.ShutdownHookManager: Shutdown hook called\n",
      "21/12/10 23:13:32 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-c28de3e3-efb6-4b80-8bc5-481d338cc941/pyspark-a8f1df60-4c04-48e6-846b-437f59d8cf4d\n",
      "21/12/10 23:13:32 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-e47b88cf-ef45-4b83-baff-9e39316b101e\n",
      "21/12/10 23:13:32 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-c28de3e3-efb6-4b80-8bc5-481d338cc941\n"
     ]
    }
   ],
   "source": [
    "#运行刚才写好的读数据代码，查看是否写入成功\n",
    "!spark-submit /home/spark/pyspark-pro/SparkOperateHBase.py | less"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9439c5c",
   "metadata": {},
   "source": [
    "## 综合案例\n",
    "### 案例1:求TOP值\n",
    "#### 任务描述\n",
    "\n",
    "orderid, userid, payment, productid\n",
    "file1.txt \n",
    "```shell\n",
    "1,1768,50,155 \n",
    "2,1218, 600,211 \n",
    "3,2239,788,242 \n",
    "4,3101,28,599 \n",
    "5,4899,290,129 \n",
    "6,3110,54,1201\n",
    "7,4436,259,877 \n",
    "8,2369,7890,27\n",
    "```\n",
    "\n",
    "file2.txt\n",
    "```shell\n",
    "100,4287,226,233 \n",
    "101,6562,489,124 \n",
    "102,1124,33,17 \n",
    "103,3267,159,179 \n",
    "104,4569,57,125\n",
    "105,1438,37,116\n",
    "```\n",
    "\n",
    "求TOP N个payment值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cc67031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7890, 788, 600, 489, 290]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "# conf = SparkConf().setMaster('local').setAppName('ReadHBase')\n",
    "# sc = SparkContext(conf=conf)\n",
    "lines = sc.textFile('file:///opt/code/SparkProgramming-PySpark/RDD-p1')\n",
    "result1 = lines.filter(lambda line: len(line.strip())>0 and (len(line.split(\",\")) == 4))\n",
    "result2 = result1.map(lambda x: x.split(',')[2])\n",
    "result3 = result2.map(lambda x: (int(x), ''))\n",
    "result4 = result3.repartition(1).sortByKey(False)\n",
    "result5 = result4.map(lambda x: x[0])\n",
    "result5.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2d961c",
   "metadata": {},
   "source": [
    "或者，你可以写成脚本\n",
    "\n",
    "```python\n",
    "#!/home/spark/miniconda3/envs/bigdata/env/bin/python\n",
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf().setMaster('local').setAppName('ReadHBase')\n",
    "sc = SparkContext(conf=conf)\n",
    "lines = sc.textFile('file:///opt/code/SparkProgramming-PySpark/RDD-p1')\n",
    "result1 = lines.filter(lambda line: len(line.strip())>0 and (len(line.split(\",\")) == 4))\n",
    "result2 = result1.map(lambda x: x.split(',')[2])\n",
    "result3 = result2.map(lambda x: (int(x), ''))\n",
    "result4 = result3.repartition(1).sortByKey(False)\n",
    "result5 = result4.map(lambda x: x[0]).take(5)\n",
    "\n",
    "for a in result5:\n",
    "    print(a)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335aedc4",
   "metadata": {},
   "source": [
    "### 任务2：文件排序\n",
    "#### 任务描述\n",
    "\n",
    "有多个输入文件，每个文件中的每一行内容均为一个整数，进行排序后，输出到一个新的文件中，输出的内容个数为每行两个整数，第一个整数为第二个整数的排序位次，第二个整数为原待排序的整数\n",
    "\n",
    "file1.txt\n",
    "```\n",
    "33\n",
    "37\n",
    "12\n",
    "40\n",
    "```\n",
    "\n",
    "file2.txt\n",
    "```\n",
    "4\n",
    "16\n",
    "39\n",
    "5\n",
    "```\n",
    "\n",
    "file3.txt\n",
    "```\n",
    "1\n",
    "45\n",
    "25\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d8917069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1), (2, 4), (3, 5), (4, 12), (5, 16), (6, 25), (7, 33), (8, 37), (9, 39), (10, 40), (11, 45)]\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "\n",
    "def getindex():\n",
    "    global index\n",
    "    index += 1\n",
    "    return index\n",
    "\n",
    "def main():\n",
    "    lines  = sc.textFile('file:///opt/code/SparkProgramming-PySpark/RDD-p2/')\n",
    "    index = 0\n",
    "    result = lines.filter(lambda line:(len(line.strip()) > 0)).map(lambda x: (int(x.strip()), ''))\n",
    "    result2 = result.repartition(1).sortByKey(True)\n",
    "    result3 = result2.map(lambda x: x[0]).map(lambda x: (getindex(), x))\n",
    "    print(result3.collect())\n",
    "    result3.saveAsTextFile('file:///opt/code/SparkProgramming-PySpark/sortresult')\n",
    "    \n",
    "    \n",
    "main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "66b94155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1)\r\n",
      "(2, 4)\r\n",
      "(3, 5)\r\n",
      "(4, 12)\r\n",
      "(5, 16)\r\n",
      "(6, 25)\r\n",
      "(7, 33)\r\n",
      "(8, 37)\r\n",
      "(9, 39)\r\n",
      "(10, 40)\r\n",
      "(11, 45)\r\n"
     ]
    }
   ],
   "source": [
    "!cat /opt/code/SparkProgramming-PySpark/sortresult/pa*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
