{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c52a9b9a",
   "metadata": {},
   "source": [
    "# Spark MLlib\n",
    "## Spark MLlib简介\n",
    "### 什么是机器学习\n",
    "\n",
    "机器学习是一门人工智能科学，该领域的主要研究对象是人工智能。机器学习利用数据或以往的经验，以此优化计算机程序的性能标准  \n",
    "机器学习强调三个关键词：算法、经验、性能\n",
    "\n",
    "### 基于大数据的机器学习\n",
    "\n",
    "- 传统的机器学习算法，由于技术和单机存储的限制，只能在少量数据上使用，依赖数据抽样\n",
    "- 大数据技术的出现，可以支持在全量数据上进行机器学习\n",
    "- 机器学习算法涉及大量迭代计算\n",
    "- 基于磁盘的MapReduce不适合进行大量迭代计算\n",
    "- 基于内存的Spark比较适合进行大量迭代计算\n",
    "\n",
    "### Spark机器学习库MLlib\n",
    "\n",
    "- Spark提供了一个基于海量数据的机器学习库，它提供了常用机器学习算法的分布式实现\n",
    "- 开发者只需要有Spark基础并且了解机器学习算法的原理，以及方法相关参数的含义，就可以轻松的调用相应的API来实现基于海量数据的机器学习过程\n",
    "- pyspark的即席查询也是一个关键。算法工程师可以编写代码边看结果\n",
    "- 需要注意的是，MLlib中只包含能够在集群上运行良好的并行算法，这一点很重要\n",
    "- 一些较新的研究得出的算法因为使用于集群，也被包含在MLlib中，例如分布式随机森林算法、最小交替二乘法。\n",
    "- 如果是小规模数据集上训练各机器学习模型，可以是用单节点的机器学习算法库（Weka）\n",
    "- MLlib由一些通用的机器学习算法和工具组成，包括分类、回归、聚类、协同过滤、降维等,同时还包括底层的优化原语和高层的流水线（Pipeline）API，具体如下：\n",
    "    - 算法工具：常用的学习算法，如分类、回归、聚类和协同过滤\n",
    "    - 特征化工具：特征提取、转化、降维和选择工具\n",
    "    - 流水线(Pipeline)：用于构建、评估和调整机器学习工作流的工具\n",
    "    - 持久性：保存和加载算法、模型和管道\n",
    "    - 实用工具：线性代数、统计、数据处理等工具\n",
    "    \n",
    "    \n",
    "    \n",
    "MLlib目前支持4中常见的机器学习问题：分类、回归、聚类和协同过滤\n",
    "    \n",
    "    \n",
    "  ||离散数据|连续数据|  \n",
    "  |--|--|--| \n",
    "  |监督学习|Classification、LogisticRegression(with Elastic-Net)、SVM、DecisionTree、RandomForest、GBT、NaiveBayes、MultilayerPerceptron、OneVsRest| Regression、LogisticRegression(with Elastic-Net)、DecsionTree、RandomFores、GBT、AFTSurvivalRegression、IsotonicRegression|\n",
    "  |无监督学习|Clustering、KMeans、GuassianMixture、LDA、PowerIterationClustering、BisectingKMeans| Dimensionality Reduction、 matrix factorization、PCA、SVD、ALS、WLS|\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eec12ce",
   "metadata": {},
   "source": [
    "## 机器学习流水线\n",
    "### 机器学习流水线概念\n",
    "- DataFrame：使用Spark SQL中的DataFrame作为数据集，它可以容纳各种数据类型。较之RDD，DataFrame包含了schema信息，更类似传统数据库的二维表格。\n",
    "- 它被ML Pipeline用来存储源数据。例如，DataFrame中的列可以是存储的文本、特征向量、真实标签和预测的标签等\n",
    "\n",
    "- Transformer：转换器，是一种可以将一个DataFrame转换为另一个DataFrame的算法。比如一个模型就是一个Transformer。它可以把一个不包含预测标签等测试数据集DataFrame打上标签，转换成另一个包含预测标签的DataFrame\n",
    "- Estimator：估计器或评估器，它是学习算法或在训练数据上的训练方法的概念抽象。在Pipeline里通常是被用来操作DataFrame数据并生成一个Transformer。从技术上讲，Estimator实现了一个方法fit()，它接受一个DataFrame并产生一个转换器。比如一个随机森林算法就是一个Estimator，它可以调用fit()，通过训练特征数据而得到一个随机深林算法\n",
    "- Parameter：Parameter被用来设置Transformer或者Estimator的参数。现在，所有转换器和评估器可共享用于指定参数的公共API。ParamMap是一组（参数，值）对\n",
    "- PipeLine：流水线或者管道。流水线将多个工作流阶段（转换器和评估器）连接在一起，形成机器学习的工作流，并获得结果输出。\n",
    "\n",
    "\n",
    "### 流水线工作流程\n",
    "要构建一个Pipeline流水线，首先需要定义Pipeline中的各个流水线阶段PipelineStage(包括转换器和评估器），比如指标提取和转换模型训练等。有了这些处理特定问题的转换器和评估器，就可以按照具体的处理逻辑有序地组织PipelineStages并创建一个Pipeline\n",
    "```python\n",
    "pipeline = Pipeline(stages=[stage1, stage2, stage3]\n",
    "```\n",
    "\n",
    "然后就可以把训练数据集作为输入参数，调用Pipeline实例的fit方法来开始以流的方式来处理源训练数据。这个调用会返回一个PipelineModel类实例，进而被用来预测测试数据的标签\n",
    "\n",
    "- 流水线的各个阶段按顺序运行，输入的DataFrame在它通过每个阶段时被转换\n",
    "\n",
    "值得注意的是，流水线本身也可以看作是一个评估器，在流水线的fit()方法运行之后，它产生一个PipelineModel，它是一个Transformer。这个管道模型将在测试数据的时候使用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402f7d62",
   "metadata": {},
   "source": [
    "### 构建一个机器学习流水线\n",
    "以逻辑斯蒂回归为例，构建一个典型的机器学习过程\n",
    "\n",
    "**任务描述**  \n",
    "查找出所有包含“spark”的句子，将包含“spark”的句子的标签设为1，没有“spark”的句子的标签设为0\n",
    "- 需要使用SparkSession对象\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f7df5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master('local').appName('WordCount').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211524f5",
   "metadata": {},
   "source": [
    "- 引入要包含的包并构建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "750b3b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spark/miniconda3/envs/bigdata/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "\n",
    "#Prepare training documents from a list of (id, text, label) tuples\n",
    "training = spark.createDataFrame([\n",
    "    (0, 'a b c d e spark', 1.0),\n",
    "    (1, 'b d', 0.0),\n",
    "    (2, 'spark f g h', 1.0),\n",
    "    (3, 'hadoop mapreduce', 0.0),\n",
    "], ['id', 'text', 'label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b6fc5c",
   "metadata": {},
   "source": [
    "- 定义Pipeline中的各个流水线阶段PipelineStage，包括转换器和评估器，具体地，包含tokenizer，hashing TF和lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44a641d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol='text', outputCol='words')\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol='features')\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34036be",
   "metadata": {},
   "source": [
    "- 按照具体的处理逻辑有序地组织PipelineStages，并创建一个Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "405b7fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62622c21",
   "metadata": {},
   "source": [
    "现在构建的Pipeline本质上是一个Estimator，在它的fit()方法运行之后，它将产生一个PipelineModel，它是一个Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50d8f9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c3cced",
   "metadata": {},
   "source": [
    "可以看到，model的类型是一个PipelineModel，这个流水线模型将在测试数据的时候使用\n",
    "\n",
    "\n",
    "- 构建测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1957aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = spark.createDataFrame([\n",
    "    (4, 'spark i j k'),\n",
    "    (5, 'I m n'),\n",
    "    (6, 'spark hadoop spark'),\n",
    "    (7, 'apache hadoop')\n",
    "], ['id', 'text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a029b80b",
   "metadata": {},
   "source": [
    "- 调用之前训练好的PipelineModel的transform()方法，让测试数据按顺序通过拟合的流水线，生成预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f18f1c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, spark i j k) --> prob=[0.1596407738787475,0.8403592261212525], prediction=1.000000\n",
      "(5, I m n) --> prob=[0.8378325685476744,0.16216743145232562], prediction=0.000000\n",
      "(6, spark hadoop spark) --> prob=[0.06926633132976037,0.9307336686702395], prediction=1.000000\n",
      "(7, apache hadoop) --> prob=[0.9821575333444218,0.01784246665557808], prediction=0.000000\n"
     ]
    }
   ],
   "source": [
    "prediction = model.transform(test)\n",
    "selected = prediction.select('id', 'text', 'probability', 'prediction')\n",
    "for row in selected.collect():\n",
    "    rid, text, prob, prediction = row\n",
    "    print(\"(%d, %s) --> prob=%s, prediction=%f\" % (rid, text, str(prob), prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd86094",
   "metadata": {},
   "source": [
    "## 特征提取和转换\n",
    "### 特征提取\n",
    "\n",
    "- “词频-逆向文件频率”(TF-IDF)是一种在文本挖掘中广泛使用的特征向量化方法，它可以体现一个文档中词语在语料库中的重要程度\n",
    "- 词语由t表示，文档由d表示，语料库由D表示。词频TF(t, d)是词语t在文档d中出现的次数。文件频率DF(t, D)是包含词语的文档的个数\n",
    "- TF-IDF就是在数值化文档信息，衡量词语能够提供多少信息以区分文档。其定义如下:\n",
    "$$\n",
    "        IDF(t,D) = log \\frac {|D| + 1}{DF(t,D)+1}\\\n",
    "\n",
    "        TFIDF(t,d,D) = TF(t,d)·IDF(t,D)\n",
    "$$\n",
    "\n",
    "在Spark ML库中，TF-IDF被分成两部分：\n",
    "- TF(+hashing)\n",
    "    HashingTF是一个Transformer，在文本处理中，接收词条的集合然后把这些集合转化为固定长度的特征向量。这个算法在哈希的同时会统计各个词条的词频。\n",
    "        \n",
    "- IDF\n",
    "    IDF是一个Estimator，在一个数据集上应用它的fit()方法，产生一个IDFModel。该IDFModel接收特征向量（由HashingTF产生），然后计算每一个词在文档中出现的频次。IDF会减少那些在语料库中出现频率较高的词的权重。\n",
    "\n",
    "**过程描述**\n",
    "- 在下面的代码段中，我们以一组句子开始\n",
    "- 首先使用分解器Tokenizer把句子划分为单个词语\n",
    "- 对每一个句子（词带），使用HashingTF将句子转换为特征向量\n",
    "- 最后使用IDF重新调整特征向量（这种转换通常可以提高使用文本特征的性能）\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa805c43",
   "metadata": {},
   "source": [
    "(1)导入TF-IDF所需要的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b3772cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7993f1fc",
   "metadata": {},
   "source": [
    "(2)创建一个简单的DataFrame，每一个句子代表一个文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ede4c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceData = spark.createDataFrame([\n",
    "    (0, 'I heard about Spark and I love Spark'),\n",
    "    (0, 'I wish Java could use case classes'),\n",
    "    (1, 'Logistic regression models are neat')\n",
    "]).toDF('label', 'sentence')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbe3a89",
   "metadata": {},
   "source": [
    "(3)得到文档集合后，即可用tokenizer对句子进行分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8a4bd9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|            sentence|               words|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|I heard about Spa...|[i, heard, about,...|\n",
      "|    0|I wish Java could...|[i, wish, java, c...|\n",
      "|    1|Logistic regressi...|[logistic, regres...|\n",
      "+-----+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol='sentence', outputCol='words')\n",
    "wordsData = tokenizer.transform(sentenceData)\n",
    "wordsData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42d7de5",
   "metadata": {},
   "source": [
    "(4)得到分词后的文档序列后，即可使用HashingTF的transform()方法把句子哈希成特征向量，这里设置哈希表的桶数为2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e39dc2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+---------------------------------------------------------------------+\n",
      "|words                                        |rawFeatures                                                          |\n",
      "+---------------------------------------------+---------------------------------------------------------------------+\n",
      "|[i, heard, about, spark, and, i, love, spark]|(2000,[240,333,1105,1329,1357,1777],[1.0,1.0,2.0,2.0,1.0,1.0])       |\n",
      "|[i, wish, java, could, use, case, classes]   |(2000,[213,342,489,495,1329,1809,1967],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|[logistic, regression, models, are, neat]    |(2000,[286,695,1138,1193,1604],[1.0,1.0,1.0,1.0,1.0])                |\n",
      "+---------------------------------------------+---------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashingTF = HashingTF(inputCol='words', outputCol='rawFeatures', numFeatures=2000)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "featurizedData.select('words', 'rawFeatures').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da51defb",
   "metadata": {},
   "source": [
    "(5)调用IDF方法来构造特征向量的规模，生成的变量idf是一个评估器，在特征向量上应用它的fit()方法，会产生一个IDFModel（名称为idfModel）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0434880e",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol='rawFeatures', outputCol='features')\n",
    "idfModel = idf.fit(featurizedData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32e8da8",
   "metadata": {},
   "source": [
    "(6)调用IDFModel的transform()方法，可以得到每一个单词对应的TF-IDF度量值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cab0a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|features                                                                                                                                                                       |label|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|(2000,[240,333,1105,1329,1357,1777],[0.6931471805599453,0.6931471805599453,1.3862943611198906,0.5753641449035617,0.6931471805599453,0.6931471805599453])                       |0    |\n",
      "|(2000,[213,342,489,495,1329,1809,1967],[0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.28768207245178085,0.6931471805599453,0.6931471805599453])|0    |\n",
      "|(2000,[286,695,1138,1193,1604],[0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453])                                               |1    |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rescaledData = idfModel.transform(featurizedData)\n",
    "rescaledData.select('features', 'label').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd02bbc",
   "metadata": {},
   "source": [
    "### 特征转换：标签和和索引的转换\n",
    "\n",
    "- 在机器学习处理过程中，为了方便相关算法的实现，经常需要把标签数据（一般是字符串）转化成整数索引，或是在计算结束后将整数索引还原为相应的标签\n",
    "- Spark ML包中提供了几个相关的转换器，例如，StringIndexer、IndexToString、OneHotEncoder、VectorIndexer，它们提供了十分方便的特征转换功能，这些转换器都位于org.apache.spark.ml.feature包下\n",
    "- 值得注意的是，用于特征转换的转换器和其他的机器学习算法一样，也属于ML Pipeline模型的一部分，可以用来构成机器学习流水线，以StringIndexer为例，其存储着进行标签数值化过程的相关超参数，是一个Estimator，对其调用fit()方法即可生成相应的模型StringIndexModel类，很显然，它存储于用于DataFrame进行相关处理的参数，是一个Transformer\n",
    "\n",
    "- StringIndexer\n",
    "    - StringIndexer转换器可以把一列类别型的特征（或标签）进行编码，使其数值化，索引的范围从0开始，该过程可以使得相应的特征索引化，使得某些无法接受类别特征的算法可以使用，并提高诸如决策树等机器学习算法等效率\n",
    "    - 索引构建的顺序为标签的频率，优先编码频率较大的标签，所以出现频率最高的标签为0号\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c99b55",
   "metadata": {},
   "source": [
    "(1)首先，引入所需要使用的类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e938f948",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76317504",
   "metadata": {},
   "source": [
    "(2)其次，构建1个DataFrame，设置StringIndexer的输入列和输出列的名字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73a68b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master('local').getOrCreate()\n",
    "df = spark.createDataFrame([(0, 'a'), (1, 'b'), (2, 'c'), (3, 'a'), (4, 'a'), (5, 'c')],\n",
    "                          ['id', 'category'])\n",
    "indexer = StringIndexer(inputCol='category', outputCol='categoryIndex')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d855e6",
   "metadata": {},
   "source": [
    "(3)然后，通过fit()方法进行模型训练，用训练出的模型对原数据集进行处理，并通过indexed.show()进行展示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "315bd09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+\n",
      "| id|category|categoryIndex|\n",
      "+---+--------+-------------+\n",
      "|  0|       a|          0.0|\n",
      "|  1|       b|          2.0|\n",
      "|  2|       c|          1.0|\n",
      "|  3|       a|          0.0|\n",
      "|  4|       a|          0.0|\n",
      "|  5|       c|          1.0|\n",
      "+---+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = indexer.fit(df)\n",
    "indexed = model.transform(df)\n",
    "indexed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4118d5",
   "metadata": {},
   "source": [
    "- IndexToString\n",
    "    - 与StringIndexer相对应，IndexToString的作用是把标签索引的一列重新映射回原有的字符型标签\n",
    "    - 其主要使用场景一般都是和StringIndexer配合，先用StringIndexer将标签转化成标签索引，进行模型训练，然后在预测标签的时候再把标签索引转化成原有的字符标签\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7550a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+\n",
      "| id|originalCategory|\n",
      "+---+----------------+\n",
      "|  0|               a|\n",
      "|  1|               b|\n",
      "|  2|               c|\n",
      "|  3|               a|\n",
      "|  4|               a|\n",
      "|  5|               c|\n",
      "+---+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import IndexToString, StringIndexer\n",
    "toString = IndexToString(inputCol='categoryIndex', outputCol='originalCategory')\n",
    "indexString = toString.transform(indexed)\n",
    "indexString.select('id', 'originalCategory').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99f16d4",
   "metadata": {},
   "source": [
    "- VectorIndexer\n",
    "    - 之前介绍的StringIndexer是针对单个类别型特征进行转换，倘若所有特征都已经被组织在一个向量中，又想对其中某些单个分量进行处理时，Spark ML提供了VectorIndexer类来解决向量数据集中的类别性特征转换\n",
    "    - 通过为其提供maxCategories超参数，它可以自动识别哪些特征是类别型的，并且将原始值转换为类别索引。它基于不同特征值的数量来识别哪些特征需要被类别化，那些取值可能性最多不超过maxCategories的特征需要会被认为是类别型的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c555423",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.linalg import Vector, Vectors\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (Vectors.dense(-1.0, 1.0, 1.0),),\n",
    "    (Vectors.dense(-1.0, 3.0, 1.0),),\n",
    "    (Vectors.dense(0.0, 5.0, 1.0),)], ['features'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2308c8",
   "metadata": {},
   "source": [
    "然后，构建VectorIndexer转换器，设置输入和输出列，并进行模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f338ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = VectorIndexer(inputCol='features', outputCol='indexed', maxCategories=2)\n",
    "indexerModel = indexer.fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbbf2ba",
   "metadata": {},
   "source": [
    "接下来，通过VectorIndexerModel的categoryMaps成员来获得被转换的特征及其映射，这里可以看到，共有两个特征被转换，分别是0号和2号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23ed511a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose 2categorical features: KeysView({0: {0.0: 0, -1.0: 1}, 2: {1.0: 0}})\n"
     ]
    }
   ],
   "source": [
    "categoricalFeatures = indexerModel.categoryMaps.keys()\n",
    "print('Choose '+ str(len(categoricalFeatures)) + 'categorical features: ' + str(categoricalFeatures))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0217ff",
   "metadata": {},
   "source": [
    "最后，把模型应用于原有的数据，并打印结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecfd4840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+\n",
      "|      features|      indexed|\n",
      "+--------------+-------------+\n",
      "|[-1.0,1.0,1.0]|[1.0,1.0,0.0]|\n",
      "|[-1.0,3.0,1.0]|[1.0,3.0,0.0]|\n",
      "| [0.0,5.0,1.0]|[0.0,5.0,0.0]|\n",
      "+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexed = indexerModel.transform(df)\n",
    "indexed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39826af8",
   "metadata": {},
   "source": [
    "## 分类与回归\n",
    "### 逻辑斯蒂回归分类器\n",
    "逻辑斯蒂回归(logistic regression)是统计学习中的经典分类方法，属于对数线性模型。logistic回归的因变量可以是二分类也可以是多分类。\n",
    "\n",
    "任务描述：以iris数据集（iris）为例进行分析  \n",
    "iris以鸢尾花的特征作为数据来源，数据集包含150个数据集，分为3类，每类50个数据，每个数据包含4个属性，是在数据挖掘、数据分类中非常常用的测试集、训练集。为了便于理解，这里主要用后两个属性（花瓣的长度和宽度）来进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2c708e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-12-13 19:30:32--  http://dblab.xmu.edu.cn/blog/wp-content/uploads/2017/03/iris.txt\n",
      "Resolving dblab.xmu.edu.cn (dblab.xmu.edu.cn)... 210.34.0.35, 2001:da8:e800::35\n",
      "Connecting to dblab.xmu.edu.cn (dblab.xmu.edu.cn)|210.34.0.35|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/plain]\n",
      "Saving to: ‘iris.txt’\n",
      "\n",
      "    [ <=>                                   ] 4,698       --.-K/s   in 0.001s  \n",
      "\n",
      "2021-12-13 19:30:32 (5.81 MB/s) - ‘iris.txt’ saved [4698]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://dblab.xmu.edu.cn/blog/wp-content/uploads/2017/03/iris.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4e3088",
   "metadata": {},
   "source": [
    "首先我们先取其中的后两类数据，用二项逻辑斯蒂回归进行二分类分析\n",
    "\n",
    "第1步：导入本地向量Vector和Vectors，导入所需要的类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faf4ee90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vector, Vectors\n",
    "from pyspark.sql import Row, functions\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, HashingTF, Tokenizer\n",
    "from pyspark.ml.classification import LogisticRegression, LogisticRegressionModel, \\\n",
    "                    BinaryLogisticRegressionSummary\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8737f52",
   "metadata": {},
   "source": [
    "第2步：我们制定一个函数，来返回一个指定的数据然后读取文本文件，第一个map把每行的数据用“,”隔开，比如在我们的数据集中，每行被分成了5部分，前4部分是鸢尾花的4个特征，最后一部分是鸢尾花的分类；我们在这里把特征存储在Vector中，创建一个Iris模式的RDD，然后转化成dataframe；最后调用show()来查看部分数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d8564f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------+\n",
      "|         features|      label|\n",
      "+-----------------+-----------+\n",
      "|[5.1,3.5,1.4,0.2]|Iris-setosa|\n",
      "|[4.9,3.0,1.4,0.2]|Iris-setosa|\n",
      "|[4.7,3.2,1.3,0.2]|Iris-setosa|\n",
      "|[4.6,3.1,1.5,0.2]|Iris-setosa|\n",
      "|[5.0,3.6,1.4,0.2]|Iris-setosa|\n",
      "|[5.4,3.9,1.7,0.4]|Iris-setosa|\n",
      "|[4.6,3.4,1.4,0.3]|Iris-setosa|\n",
      "|[5.0,3.4,1.5,0.2]|Iris-setosa|\n",
      "|[4.4,2.9,1.4,0.2]|Iris-setosa|\n",
      "|[4.9,3.1,1.5,0.1]|Iris-setosa|\n",
      "|[5.4,3.7,1.5,0.2]|Iris-setosa|\n",
      "|[4.8,3.4,1.6,0.2]|Iris-setosa|\n",
      "|[4.8,3.0,1.4,0.1]|Iris-setosa|\n",
      "|[4.3,3.0,1.1,0.1]|Iris-setosa|\n",
      "|[5.8,4.0,1.2,0.2]|Iris-setosa|\n",
      "|[5.7,4.4,1.5,0.4]|Iris-setosa|\n",
      "|[5.4,3.9,1.3,0.4]|Iris-setosa|\n",
      "|[5.1,3.5,1.4,0.3]|Iris-setosa|\n",
      "|[5.7,3.8,1.7,0.3]|Iris-setosa|\n",
      "|[5.1,3.8,1.5,0.3]|Iris-setosa|\n",
      "+-----------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    rel = {}\n",
    "    rel['features'] = Vectors.dense(float(x[0]), float(x[1]), float(x[2]), float(x[3]))\n",
    "    rel['label'] = str(x[4])\n",
    "    return rel\n",
    "\n",
    "spark = SparkSession.builder.master('local').getOrCreate()\n",
    "data = spark.sparkContext.textFile('file:///home/spark/code/SparkProgramming-PySpark/studying/iris.txt')\\\n",
    "        .map(lambda line: line.split(',')).map(lambda p: Row(**f(p))).toDF()\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e62b849",
   "metadata": {},
   "source": [
    "第3步：分别获取标签列和特征列，进行索引并进行重命名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b074893a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelIndexer = StringIndexer().setInputCol('label').setOutputCol('indexedLabel').fit(data)\n",
    "featureIndexer = VectorIndexer().setInputCol('features').setOutputCol('indexedFeatures').fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e77cef",
   "metadata": {},
   "source": [
    "第4步：设置LogisticRegression算法的参数。这里设置了循环次数为100次，规范化项为0.3等，具体可以设置的参数，可以通过explainParams()来获取，还能看到程序已经设置的参数的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "529d42a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression parameters: \n",
      "aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0, current: 0.8)\n",
      "family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)\n",
      "featuresCol: features column name. (default: features, current: indexedFeatures)\n",
      "fitIntercept: whether to fit an intercept term. (default: True)\n",
      "labelCol: label column name. (default: label, current: indexedLabel)\n",
      "lowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n",
      "lowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained optimization. The bounds vector size must beequal with 1 for binomial regression, or the number oflasses for multinomial regression. (undefined)\n",
      "maxIter: max number of iterations (>= 0). (default: 100, current: 100)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0, current: 0.3)\n",
      "standardization: whether to standardize the training features before fitting the model. (default: True)\n",
      "threshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
      "upperBoundsOnCoefficients: The upper bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n",
      "upperBoundsOnIntercepts: The upper bounds on intercepts if fitting under bound constrained optimization. The bound vector size must be equal with 1 for binomial regression, or the number of classes for multinomial regression. (undefined)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression().setLabelCol('indexedLabel').setFeaturesCol('indexedFeatures')\\\n",
    "            .setMaxIter(100).setRegParam(0.3).setElasticNetParam(0.8)\n",
    "print('LogisticRegression parameters: \\n' + lr.explainParams())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a4ff89",
   "metadata": {},
   "source": [
    "第5步：设置一个IndexToString的转换器，把预测的类别重新转化成字符型的。构建一个机器学习流水线，设置各个阶段."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e23013e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelConverter = IndexToString().setInputCol('prediction').setOutputCol('predictionLabel')\\\n",
    "            .setLabels(labelIndexer.labels)\n",
    "LrPipeline = Pipeline().setStages([labelIndexer, featureIndexer, lr, labelConverter])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b96d5e9",
   "metadata": {},
   "source": [
    "第6步：把数据集随机分成训练集和测试集，其中训练集占70%。Pipeline本质就是一个评估器，当Pipeline调用fit()的时候就产生了一个PipelineModel就可以调用transform()来预测，生成一个新的DataFrame，即利用训练得到的模型对测试集进行验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c77056e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData, testData = data.randomSplit([0.7, 0.3])\n",
    "lrPipelineModel = LrPipeline.fit(trainingData)\n",
    "lrPredictions= lrPipelineModel.transform(testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9918c43",
   "metadata": {},
   "source": [
    "第7步：输出预测结果，其中，select选择要输出的列，collect获取所有行的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b127822",
   "metadata": {},
   "outputs": [],
   "source": [
    "preRel = lrPredictions.select(\n",
    "    'predictionLabel', 'label', 'features', 'probability'\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcc75aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris-setosa,[4.3,3.0,1.1,0.1]-->prob=[0.5422572686691972,0.26976850602033237,0.1879742253104703], predictionLabelIris-setosa\n",
      "Iris-setosa,[4.4,2.9,1.4,0.2]-->prob=[0.5138979821868874,0.2825238997163162,0.2035781180967964], predictionLabelIris-setosa\n",
      "Iris-setosa,[4.4,3.2,1.3,0.2]-->prob=[0.5200663364012367,0.2789388343933832,0.20099482920538006], predictionLabelIris-setosa\n",
      "Iris-setosa,[4.6,3.2,1.4,0.2]-->prob=[0.5138979821868874,0.2825238997163162,0.2035781180967964], predictionLabelIris-setosa\n",
      "Iris-setosa,[4.7,3.2,1.3,0.2]-->prob=[0.5200663364012367,0.2789388343933832,0.20099482920538006], predictionLabelIris-setosa\n",
      "Iris-setosa,[4.8,3.0,1.4,0.3]-->prob=[0.5039029296970303,0.2842715877055052,0.21182548259746442], predictionLabelIris-setosa\n",
      "Iris-setosa,[4.8,3.4,1.9,0.2]-->prob=[0.4830302989816677,0.3004642865379552,0.21650541448037705], predictionLabelIris-setosa\n",
      "Iris-versicolor,[4.9,2.4,3.3,1.0]-->prob=[0.32268501260340204,0.3487034070187629,0.32861158037783506], predictionLabelIris-versicolor\n",
      "Iris-setosa,[4.9,3.1,1.5,0.1]-->prob=[0.5176487499268931,0.284271419736044,0.19807983033706297], predictionLabelIris-setosa\n",
      "Iris-versicolor,[5.0,2.3,3.3,1.0]-->prob=[0.32268501260340204,0.3487034070187629,0.32861158037783506], predictionLabelIris-versicolor\n",
      "Iris-setosa,[5.0,3.0,1.6,0.2]-->prob=[0.5015504444069575,0.2897003244124621,0.20874923118058053], predictionLabelIris-setosa\n",
      "Iris-setosa,[5.0,3.3,1.4,0.2]-->prob=[0.5138979821868874,0.2825238997163162,0.2035781180967964], predictionLabelIris-setosa\n",
      "Iris-setosa,[5.1,3.3,1.7,0.5]-->prob=[0.4652286365199572,0.2976142475899676,0.23715711589007527], predictionLabelIris-setosa\n",
      "Iris-setosa,[5.1,3.8,1.9,0.4]-->prob=[0.4630254599923873,0.30327732511762795,0.2336972148899847], predictionLabelIris-setosa\n",
      "Iris-setosa,[5.2,3.5,1.5,0.2]-->prob=[0.5077253914043487,0.2861114273449431,0.20616318125070823], predictionLabelIris-setosa\n",
      "Iris-setosa,[5.2,4.1,1.5,0.1]-->prob=[0.5176487499268931,0.284271419736044,0.19807983033706297], predictionLabelIris-setosa\n",
      "Iris-setosa,[5.4,3.4,1.5,0.4]-->prob=[0.48766257223965864,0.2893625545944868,0.22297487316585454], predictionLabelIris-setosa\n",
      "Iris-setosa,[5.4,3.7,1.5,0.2]-->prob=[0.5077253914043487,0.2861114273449431,0.20616318125070823], predictionLabelIris-setosa\n",
      "Iris-setosa,[5.4,3.9,1.7,0.4]-->prob=[0.47532900460988964,0.29632841818992034,0.22834257720019], predictionLabelIris-setosa\n",
      "Iris-versicolor,[5.5,2.5,4.0,1.3]-->prob=[0.26078519734083744,0.3619774329774218,0.37723736968174076], predictionLabelIris-virginica\n",
      "Iris-setosa,[5.5,4.2,1.4,0.2]-->prob=[0.5138979821868874,0.2825238997163162,0.2035781180967964], predictionLabelIris-setosa\n",
      "Iris-versicolor,[5.6,3.0,4.1,1.3]-->prob=[0.2560514071977743,0.36429546719166606,0.3796531256105595], predictionLabelIris-virginica\n",
      "Iris-versicolor,[5.7,2.9,4.2,1.3]-->prob=[0.25137432478212535,0.36658573286349827,0.38203994235437644], predictionLabelIris-virginica\n",
      "Iris-virginica,[5.8,2.7,5.1,1.9]-->prob=[0.17133061562237512,0.36432731945362373,0.4643420649240011], predictionLabelIris-virginica\n",
      "Iris-virginica,[5.8,2.8,5.1,2.4]-->prob=[0.14152373627593423,0.34239507442280953,0.5160811893012562], predictionLabelIris-virginica\n",
      "Iris-setosa,[5.8,4.0,1.2,0.2]-->prob=[0.5262285788566725,0.2753573212424205,0.198414099900907], predictionLabelIris-setosa\n",
      "Iris-versicolor,[6.0,2.9,4.5,1.5]-->prob=[0.22238537147811335,0.36775752224801495,0.40985710627387184], predictionLabelIris-virginica\n",
      "Iris-versicolor,[6.0,3.4,4.5,1.6]-->prob=[0.21493201094067602,0.3647243363918958,0.4203436526674283], predictionLabelIris-virginica\n",
      "Iris-versicolor,[6.1,2.8,4.7,1.2]-->prob=[0.2364899820094344,0.38027633821763207,0.38323367977293354], predictionLabelIris-virginica\n",
      "Iris-virginica,[6.2,3.4,5.4,2.3]-->prob=[0.13812407775705407,0.3507062014295361,0.5111697208134097], predictionLabelIris-virginica\n",
      "Iris-virginica,[6.3,3.3,6.0,2.5]-->prob=[0.11193557833288295,0.3470779566864489,0.540986464980668], predictionLabelIris-virginica\n",
      "Iris-virginica,[6.4,2.7,5.3,1.9]-->prob=[0.16442980428931758,0.36736128467838747,0.468208911032295], predictionLabelIris-virginica\n",
      "Iris-virginica,[6.4,2.8,5.6,2.1]-->prob=[0.1431339087901536,0.36262361563448126,0.4942424755753652], predictionLabelIris-virginica\n",
      "Iris-versicolor,[6.4,2.9,4.3,1.3]-->prob=[0.2467543384150445,0.36884803984048353,0.384397621744472], predictionLabelIris-virginica\n",
      "Iris-versicolor,[6.6,2.9,4.6,1.3]-->prob=[0.23324018566814816,0.3754656269106952,0.39129418742115674], predictionLabelIris-virginica\n",
      "Iris-versicolor,[6.6,3.0,4.4,1.4]-->prob=[0.234375330365694,0.3684944175558046,0.3971302520785014], predictionLabelIris-virginica\n",
      "Iris-versicolor,[6.7,3.0,5.0,1.7]-->prob=[0.18802892444294123,0.3704561422748531,0.44151493328220565], predictionLabelIris-virginica\n",
      "Iris-versicolor,[6.7,3.1,4.7,1.5]-->prob=[0.21395913626104943,0.3717425442263137,0.41429831951263685], predictionLabelIris-virginica\n",
      "Iris-virginica,[6.8,3.0,5.5,2.1]-->prob=[0.14619034104417264,0.3613301410457863,0.49247951791004096], predictionLabelIris-virginica\n",
      "Iris-virginica,[6.8,3.2,5.9,2.3]-->prob=[0.12406676104153025,0.35642626857651416,0.5195069703819555], predictionLabelIris-virginica\n",
      "Iris-virginica,[7.1,3.0,5.9,2.1]-->prob=[0.1342830968867205,0.36636925739420123,0.4993476457190784], predictionLabelIris-virginica\n",
      "Iris-virginica,[7.4,2.8,6.1,1.9]-->prob=[0.13904454429403001,0.3785220007638051,0.48243345494216505], predictionLabelIris-virginica\n"
     ]
    }
   ],
   "source": [
    "for item in preRel:\n",
    "    print(str(item['label']) + ',' + str(item['features']) + '-->prob=' + str(item['probability']) +\\\n",
    "                                         ', predictionLabel' + str(item['predictionLabel']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7d2eeb",
   "metadata": {},
   "source": [
    "第8步：对训练的模型进行评估。创建一个Multiclass Classification Evaluator实例，用setter方法把预测分类的列名和真实的列名进行设置，然后计算预测准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbb8984f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6885304659498208"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator().setLabelCol('indexedLabel').setPredictionCol('prediction')\n",
    "lrAccuracy = evaluator.evaluate(lrPredictions)\n",
    "lrAccuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e0d811",
   "metadata": {},
   "source": [
    "第9步：通过model来获取训练得到的逻辑斯蒂模型。LrPipelineModel是一个PipelineModel，因此可以通过它的stages来获取模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28045ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      "3 X 4 CSRMatrix\n",
      "(0,2) -0.247\n",
      "(0,3) -0.2581\n",
      "(1,3) 0.3355\n",
      "Intercept: [0.7954077176700615,-0.2003011786498851,-0.5951065390201763]\n",
      "numClasses: 3\n",
      "numFeatures: 4\n"
     ]
    }
   ],
   "source": [
    "lrModel = lrPipelineModel.stages[2]\n",
    "print('Coefficients: \\n' + str(lrModel.coefficientMatrix) + '\\nIntercept: ' + str(lrModel.interceptVector)\\\n",
    "             + '\\nnumClasses: ' + str(lrModel.numClasses) + '\\nnumFeatures: ' + str(lrModel.numFeatures))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dae00b",
   "metadata": {},
   "source": [
    "### 决策树分类器\n",
    "决策树(decision tree)是一种基本的分类与回归方法，这里主要介绍用于分类的决策树。决策树呈树形结构，其中每个内部节点表示一个属性上的测试，每个分支代表一个测试输出，每个叶节点代表一种类别。学习时利用训练数据，根据损失函数最小化的原则建立决策树模型；预测时，对新的数据。利用决策树模型进行分类。  \n",
    "\n",
    "决策树学习通常包括3个步骤：特征选择、决策树的生成和决策树的剪枝\n",
    "\n",
    "继续使用鸢尾花数据集，数据集包含150个数据集，分为3类，每类50个数据，每个数据包含4个属性，是在挖掘数据、数据分类中非常常用的测试集、训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b4895ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#第1步：导入需要用的包\n",
    "from pyspark.ml.classification import DecisionTreeClassificationModel\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.linalg import Vector, Vectors\n",
    "from pyspark.sql import Row, SparkSession\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "\n",
    "spark = SparkSession.builder.master('local').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d51e3a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#第2步：读取文本文件，第一个map把每行的数据用','隔开，比如我们的数据集中，每行被分成了5部分，前4部分是鸢尾花的4个特征，最后一部分\n",
    "#是鸢尾花的分类；把特征存储在Vector中，创建一个iris模式的RDD，然后转化为DataFrame\n",
    "def f(x):\n",
    "    rel = {}\n",
    "    rel['features'] = Vectors.dense(float(x[0]), float(x[1]), float(x[2]), float(x[3]))\n",
    "    rel['label'] = str(x[4])\n",
    "    return rel\n",
    "\n",
    "data = spark.sparkContext.textFile('file:///home/spark/code/SparkProgramming-PySpark/studying/iris.txt')\\\n",
    "        .map(lambda line: line.split(',')).map(lambda p: Row(**f(p))).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c09a06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#第3步：进步处理特征和标签，把数据集随机分成训练集和测试集\n",
    "labelIndexer = StringIndexer().setInputCol('label').setOutputCol('indexedLabel').fit(data)\n",
    "featureIndexer = VectorIndexer().setInputCol('features')\\\n",
    "            .setOutputCol('indexedFeatures').setMaxCategories(4).fit(data)\n",
    "labelConverter = IndexToString().setInputCol('prediction')\\\n",
    "            .setOutputCol('predictedLabel').setLabels(labelIndexer.labels)\n",
    "trainingData, testData = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6733cc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#第4步：创建决策树模型，设置决策树参数\n",
    "dtClassifier = DecisionTreeClassifier().setLabelCol('indexedLabel').setFeaturesCol('indexedFeatures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "44d3bc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+-----------------+\n",
      "| predictedLabel|          label|         features|\n",
      "+---------------+---------------+-----------------+\n",
      "|    Iris-setosa|    Iris-setosa|[4.4,2.9,1.4,0.2]|\n",
      "|    Iris-setosa|    Iris-setosa|[4.4,3.2,1.3,0.2]|\n",
      "|    Iris-setosa|    Iris-setosa|[4.6,3.4,1.4,0.3]|\n",
      "|    Iris-setosa|    Iris-setosa|[4.8,3.0,1.4,0.1]|\n",
      "|    Iris-setosa|    Iris-setosa|[4.8,3.0,1.4,0.3]|\n",
      "|    Iris-setosa|    Iris-setosa|[4.9,3.0,1.4,0.2]|\n",
      "|    Iris-setosa|    Iris-setosa|[4.9,3.1,1.5,0.1]|\n",
      "|Iris-versicolor|Iris-versicolor|[5.0,2.0,3.5,1.0]|\n",
      "|Iris-versicolor|Iris-versicolor|[5.0,2.3,3.3,1.0]|\n",
      "|    Iris-setosa|    Iris-setosa|[5.0,3.5,1.6,0.6]|\n",
      "|    Iris-setosa|    Iris-setosa|[5.1,3.4,1.5,0.2]|\n",
      "|    Iris-setosa|    Iris-setosa|[5.1,3.5,1.4,0.3]|\n",
      "|    Iris-setosa|    Iris-setosa|[5.1,3.8,1.6,0.2]|\n",
      "|    Iris-setosa|    Iris-setosa|[5.1,3.8,1.9,0.4]|\n",
      "|    Iris-setosa|    Iris-setosa|[5.4,3.4,1.7,0.2]|\n",
      "|Iris-versicolor|Iris-versicolor|[5.5,2.3,4.0,1.3]|\n",
      "|Iris-versicolor|Iris-versicolor|[5.5,2.4,3.8,1.1]|\n",
      "|Iris-versicolor|Iris-versicolor|[5.6,2.5,3.9,1.1]|\n",
      "|Iris-versicolor|Iris-versicolor|[5.6,3.0,4.5,1.5]|\n",
      "|Iris-versicolor|Iris-versicolor|[5.7,2.6,3.5,1.0]|\n",
      "+---------------+---------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#第5步：构建机器学习流水线(Pipeline)，在训练数据集上调用fit()进行模拟训练，并在测试数据集调用transform()方法进行预测\n",
    "dtPipeline = Pipeline().setStages([labelIndexer, featureIndexer, dtClassifier, labelConverter])\n",
    "dtPipelineModel = dtPipeline.fit(trainingData)\n",
    "dtPredictions = dtPipelineModel.transform(testData)\n",
    "dtPredictions.select('predictedLabel', 'label', 'features').show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "21f9afa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9534883720930232"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator().setLabelCol('indexedLabel').setPredictionCol('prediction')\n",
    "dtAccuracy = evaluator.evaluate(dtPredictions)\n",
    "dtAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "613bc315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned classification tree model: \n",
      "DecisionTreeClassificationModel (uid=DecisionTreeClassifier_17b9f5a2b286) of depth 5 with 15 nodes\n",
      "  If (feature 2 <= 2.45)\n",
      "   Predict: 0.0\n",
      "  Else (feature 2 > 2.45)\n",
      "   If (feature 3 <= 1.65)\n",
      "    If (feature 2 <= 4.95)\n",
      "     Predict: 1.0\n",
      "    Else (feature 2 > 4.95)\n",
      "     If (feature 0 <= 6.05)\n",
      "      If (feature 1 <= 2.25)\n",
      "       Predict: 2.0\n",
      "      Else (feature 1 > 2.25)\n",
      "       Predict: 1.0\n",
      "     Else (feature 0 > 6.05)\n",
      "      Predict: 2.0\n",
      "   Else (feature 3 > 1.65)\n",
      "    If (feature 2 <= 4.85)\n",
      "     If (feature 1 <= 2.8499999999999996)\n",
      "      Predict: 2.0\n",
      "     Else (feature 1 > 2.8499999999999996)\n",
      "      Predict: 1.0\n",
      "    Else (feature 2 > 4.85)\n",
      "     Predict: 2.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#第6步：查看决策树模型结构\n",
    "treeModelClassifier = dtPipelineModel.stages[2]\n",
    "print('Learned classification tree model: \\n' + str(treeModelClassifier.toDebugString))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
